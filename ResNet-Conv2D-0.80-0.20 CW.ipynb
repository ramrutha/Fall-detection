{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 20, 3, 60)    120         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 3, 60)    240         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 20, 3, 60)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 20, 3, 64)    46144       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 3, 64)    256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 20, 3, 64)    4160        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 3, 64)    3904        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 3, 64)    256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 3, 64)    256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 20, 3, 64)    0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 20, 3, 64)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 20, 3, 64)    49216       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 3, 64)    256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 3, 64)    4160        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 3, 64)    256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 20, 3, 64)    0           re_lu_3[0][0]                    \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 20, 3, 64)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 3, 64)    49216       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 3, 64)    256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 20, 3, 64)    4160        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 3, 64)    256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 20, 3, 64)    0           re_lu_6[0][0]                    \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 20, 3, 64)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 20, 3, 128)   98432       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 3, 128)   512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 3, 128)   8320        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 3, 128)   512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 3, 128)   512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 20, 3, 128)   0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 20, 3, 128)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 3, 128)   512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 3, 128)   512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 20, 3, 128)   0           re_lu_12[0][0]                   \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 20, 3, 128)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 3, 128)   512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 3, 128)   512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 20, 3, 128)   0           re_lu_15[0][0]                   \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 20, 3, 128)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 3, 128)   512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 3, 128)   512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 20, 3, 128)   0           re_lu_18[0][0]                   \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 20, 3, 128)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 20, 3, 256)   393472      re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 3, 256)   1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 20, 3, 256)   33024       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 3, 256)   1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 3, 256)   1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 20, 3, 256)   0           batch_normalization_26[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 20, 3, 256)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 3, 256)   1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 3, 256)   1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 20, 3, 256)   0           re_lu_24[0][0]                   \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 20, 3, 256)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 3, 256)   1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 3, 256)   1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 20, 3, 256)   0           re_lu_27[0][0]                   \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 20, 3, 256)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 3, 256)   1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 3, 256)   1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 20, 3, 256)   0           re_lu_30[0][0]                   \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 20, 3, 256)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 3, 256)   1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 3, 256)   1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 20, 3, 256)   0           re_lu_33[0][0]                   \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 20, 3, 256)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 20, 3, 256)   1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 20, 3, 256)   1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 20, 3, 256)   0           re_lu_36[0][0]                   \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 20, 3, 256)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 20, 3, 512)   1573376     re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 20, 3, 512)   2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 20, 3, 512)   131584      re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 20, 3, 512)   2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 20, 3, 512)   2048        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 20, 3, 512)   0           batch_normalization_45[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 20, 3, 512)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 20, 3, 512)   3146240     re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 20, 3, 512)   2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 20, 3, 512)   2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20, 3, 512)   0           re_lu_42[0][0]                   \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 20, 3, 512)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 20, 3, 512)   3146240     re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 20, 3, 512)   2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 20, 3, 512)   2048        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 3, 512)   0           re_lu_45[0][0]                   \n",
      "                                                                 batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 20, 3, 512)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 10, 1, 512)   0           re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 5120)         0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            10242       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 28s - loss: 0.9676 - accuracy: 0.7948 - val_loss: 8.1196 - val_accuracy: 0.4706\n",
      "Epoch 2/100\n",
      "24/24 - 28s - loss: 0.6403 - accuracy: 0.8784 - val_loss: 4.7947 - val_accuracy: 0.6471\n",
      "Epoch 3/100\n",
      "24/24 - 28s - loss: 0.5735 - accuracy: 0.8758 - val_loss: 1.6525 - val_accuracy: 0.8824\n",
      "Epoch 4/100\n",
      "24/24 - 29s - loss: 0.3847 - accuracy: 0.9163 - val_loss: 0.3098 - val_accuracy: 0.9412\n",
      "Epoch 5/100\n",
      "24/24 - 26s - loss: 0.2258 - accuracy: 0.9346 - val_loss: 0.2283 - val_accuracy: 0.9059\n",
      "Epoch 6/100\n",
      "24/24 - 27s - loss: 0.2273 - accuracy: 0.9229 - val_loss: 0.8126 - val_accuracy: 0.9059\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "class_weight = {0: 0.8,\n",
    "                1: 0.2}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t95.00000%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       246\n",
      "           1       0.98      0.86      0.92       114\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.96      0.93      0.94       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
