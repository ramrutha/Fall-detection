{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "# X_train = train_X.reshape(train_X.shape[0],20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60, 1)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "# X_test = test_X.reshape(test_X.shape[0],60,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,ReLU,add,MaxPool1D,AveragePooling1D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, kernels, stride):\n",
    "        out = Conv1D(kernels, stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv1D(kernels, stride, padding='same')(out)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = 1\n",
    "    \n",
    "    inputs = Input([60,1])\n",
    "    X = Conv1D(kernels, stride)(inputs)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=3 ,stride=1):\n",
    "        #Block 1\n",
    "        out = Conv1D(kernels, 1, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv1D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv1D(kernels, 1, stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=3 ,stride=1):\n",
    "        #Block 1\n",
    "        out = Conv1D(kernels, 1, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv1D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv1D(kernels, 1, stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv1D(kernels, 1, stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = 1\n",
    "    \n",
    "    inputs = Input([60,1])\n",
    "    #Layer 1\n",
    "    X = Conv1D(60, 20, stride)(inputs)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool1D()(X)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling1D()(X)\n",
    "    X = Flatten()(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 60, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_426 (Conv1D)             (None, 41, 60)       1260        input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 41, 60)       240         conv1d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_377 (ReLU)                (None, 41, 60)       0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling1D) (None, 20, 60)       0           re_lu_377[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_428 (Conv1D)             (None, 20, 64)       11584       max_pooling1d_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 20, 64)       256         conv1d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_379 (ReLU)                (None, 20, 64)       0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_429 (Conv1D)             (None, 20, 64)       4160        re_lu_379[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_430 (Conv1D)             (None, 20, 64)       3904        max_pooling1d_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 20, 64)       256         conv1d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 20, 64)       256         conv1d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_143 (Add)                   (None, 20, 64)       0           batch_normalization_301[0][0]    \n",
      "                                                                 batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_380 (ReLU)                (None, 20, 64)       0           add_143[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_432 (Conv1D)             (None, 20, 64)       12352       re_lu_380[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 20, 64)       256         conv1d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_382 (ReLU)                (None, 20, 64)       0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_433 (Conv1D)             (None, 20, 64)       4160        re_lu_382[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 20, 64)       256         conv1d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_144 (Add)                   (None, 20, 64)       0           re_lu_380[0][0]                  \n",
      "                                                                 batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_383 (ReLU)                (None, 20, 64)       0           add_144[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_435 (Conv1D)             (None, 20, 64)       12352       re_lu_383[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 20, 64)       256         conv1d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_385 (ReLU)                (None, 20, 64)       0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_436 (Conv1D)             (None, 20, 64)       4160        re_lu_385[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 20, 64)       256         conv1d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_145 (Add)                   (None, 20, 64)       0           re_lu_383[0][0]                  \n",
      "                                                                 batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_386 (ReLU)                (None, 20, 64)       0           add_145[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_438 (Conv1D)             (None, 20, 128)      24704       re_lu_386[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 20, 128)      512         conv1d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_388 (ReLU)                (None, 20, 128)      0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_439 (Conv1D)             (None, 20, 128)      16512       re_lu_388[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_440 (Conv1D)             (None, 20, 128)      8320        re_lu_386[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 20, 128)      512         conv1d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 20, 128)      512         conv1d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_146 (Add)                   (None, 20, 128)      0           batch_normalization_311[0][0]    \n",
      "                                                                 batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_389 (ReLU)                (None, 20, 128)      0           add_146[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_442 (Conv1D)             (None, 20, 128)      49280       re_lu_389[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 20, 128)      512         conv1d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_391 (ReLU)                (None, 20, 128)      0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_443 (Conv1D)             (None, 20, 128)      16512       re_lu_391[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 20, 128)      512         conv1d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_147 (Add)                   (None, 20, 128)      0           re_lu_389[0][0]                  \n",
      "                                                                 batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_392 (ReLU)                (None, 20, 128)      0           add_147[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_445 (Conv1D)             (None, 20, 128)      49280       re_lu_392[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 20, 128)      512         conv1d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_394 (ReLU)                (None, 20, 128)      0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_446 (Conv1D)             (None, 20, 128)      16512       re_lu_394[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 20, 128)      512         conv1d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_148 (Add)                   (None, 20, 128)      0           re_lu_392[0][0]                  \n",
      "                                                                 batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_395 (ReLU)                (None, 20, 128)      0           add_148[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_448 (Conv1D)             (None, 20, 128)      49280       re_lu_395[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 20, 128)      512         conv1d_448[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_397 (ReLU)                (None, 20, 128)      0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_449 (Conv1D)             (None, 20, 128)      16512       re_lu_397[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 20, 128)      512         conv1d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_149 (Add)                   (None, 20, 128)      0           re_lu_395[0][0]                  \n",
      "                                                                 batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_398 (ReLU)                (None, 20, 128)      0           add_149[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_451 (Conv1D)             (None, 20, 256)      98560       re_lu_398[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 20, 256)      1024        conv1d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_400 (ReLU)                (None, 20, 256)      0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_452 (Conv1D)             (None, 20, 256)      65792       re_lu_400[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_453 (Conv1D)             (None, 20, 256)      33024       re_lu_398[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 20, 256)      1024        conv1d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 20, 256)      1024        conv1d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_150 (Add)                   (None, 20, 256)      0           batch_normalization_324[0][0]    \n",
      "                                                                 batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_401 (ReLU)                (None, 20, 256)      0           add_150[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_455 (Conv1D)             (None, 20, 256)      196864      re_lu_401[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 20, 256)      1024        conv1d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_403 (ReLU)                (None, 20, 256)      0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_456 (Conv1D)             (None, 20, 256)      65792       re_lu_403[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 20, 256)      1024        conv1d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_151 (Add)                   (None, 20, 256)      0           re_lu_401[0][0]                  \n",
      "                                                                 batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_404 (ReLU)                (None, 20, 256)      0           add_151[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_458 (Conv1D)             (None, 20, 256)      196864      re_lu_404[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 20, 256)      1024        conv1d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_406 (ReLU)                (None, 20, 256)      0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_459 (Conv1D)             (None, 20, 256)      65792       re_lu_406[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 20, 256)      1024        conv1d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_152 (Add)                   (None, 20, 256)      0           re_lu_404[0][0]                  \n",
      "                                                                 batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_407 (ReLU)                (None, 20, 256)      0           add_152[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_461 (Conv1D)             (None, 20, 256)      196864      re_lu_407[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 20, 256)      1024        conv1d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_409 (ReLU)                (None, 20, 256)      0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_462 (Conv1D)             (None, 20, 256)      65792       re_lu_409[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 20, 256)      1024        conv1d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_153 (Add)                   (None, 20, 256)      0           re_lu_407[0][0]                  \n",
      "                                                                 batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_410 (ReLU)                (None, 20, 256)      0           add_153[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_464 (Conv1D)             (None, 20, 256)      196864      re_lu_410[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 20, 256)      1024        conv1d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_412 (ReLU)                (None, 20, 256)      0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_465 (Conv1D)             (None, 20, 256)      65792       re_lu_412[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 20, 256)      1024        conv1d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_154 (Add)                   (None, 20, 256)      0           re_lu_410[0][0]                  \n",
      "                                                                 batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_413 (ReLU)                (None, 20, 256)      0           add_154[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_467 (Conv1D)             (None, 20, 256)      196864      re_lu_413[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 20, 256)      1024        conv1d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_415 (ReLU)                (None, 20, 256)      0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_468 (Conv1D)             (None, 20, 256)      65792       re_lu_415[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 20, 256)      1024        conv1d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_155 (Add)                   (None, 20, 256)      0           re_lu_413[0][0]                  \n",
      "                                                                 batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_416 (ReLU)                (None, 20, 256)      0           add_155[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_470 (Conv1D)             (None, 20, 512)      393728      re_lu_416[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 20, 512)      2048        conv1d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_418 (ReLU)                (None, 20, 512)      0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_471 (Conv1D)             (None, 20, 512)      262656      re_lu_418[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_472 (Conv1D)             (None, 20, 512)      131584      re_lu_416[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 20, 512)      2048        conv1d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 20, 512)      2048        conv1d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_156 (Add)                   (None, 20, 512)      0           batch_normalization_343[0][0]    \n",
      "                                                                 batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_419 (ReLU)                (None, 20, 512)      0           add_156[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_474 (Conv1D)             (None, 20, 512)      786944      re_lu_419[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 20, 512)      2048        conv1d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_421 (ReLU)                (None, 20, 512)      0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_475 (Conv1D)             (None, 20, 512)      262656      re_lu_421[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 20, 512)      2048        conv1d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_157 (Add)                   (None, 20, 512)      0           re_lu_419[0][0]                  \n",
      "                                                                 batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_422 (ReLU)                (None, 20, 512)      0           add_157[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_477 (Conv1D)             (None, 20, 512)      786944      re_lu_422[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 20, 512)      2048        conv1d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_424 (ReLU)                (None, 20, 512)      0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_478 (Conv1D)             (None, 20, 512)      262656      re_lu_424[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 20, 512)      2048        conv1d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_158 (Add)                   (None, 20, 512)      0           re_lu_422[0][0]                  \n",
      "                                                                 batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_425 (ReLU)                (None, 20, 512)      0           add_158[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_4 (AveragePoo (None, 10, 512)      0           re_lu_425[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 5120)         0           average_pooling1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 2)            10242       flatten_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,743,198\n",
      "Trainable params: 4,726,054\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 7s - loss: 1.2303 - accuracy: 0.7490 - val_loss: 9.0867 - val_accuracy: 0.3882\n",
      "Epoch 2/100\n",
      "24/24 - 6s - loss: 0.6397 - accuracy: 0.8314 - val_loss: 2.5641 - val_accuracy: 0.5882\n",
      "Epoch 3/100\n",
      "24/24 - 6s - loss: 0.4519 - accuracy: 0.8641 - val_loss: 1.8790 - val_accuracy: 0.8353\n",
      "Epoch 4/100\n",
      "24/24 - 6s - loss: 0.3392 - accuracy: 0.9281 - val_loss: 0.6826 - val_accuracy: 0.8824\n",
      "Epoch 5/100\n",
      "24/24 - 6s - loss: 0.2653 - accuracy: 0.9464 - val_loss: 1.0669 - val_accuracy: 0.8471\n",
      "Epoch 6/100\n",
      "24/24 - 6s - loss: 0.2860 - accuracy: 0.9203 - val_loss: 0.6162 - val_accuracy: 0.8353\n",
      "Epoch 7/100\n",
      "24/24 - 6s - loss: 0.1807 - accuracy: 0.9477 - val_loss: 1.1962 - val_accuracy: 0.7882\n",
      "Epoch 8/100\n",
      "24/24 - 6s - loss: 0.3839 - accuracy: 0.9477 - val_loss: 1.4892 - val_accuracy: 0.8706\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t92.22222%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       246\n",
      "           1       0.91      0.83      0.87       114\n",
      "\n",
      "    accuracy                           0.92       360\n",
      "   macro avg       0.92      0.90      0.91       360\n",
      "weighted avg       0.92      0.92      0.92       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
