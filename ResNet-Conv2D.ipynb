{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 20, 3, 60)    120         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 3, 60)    240         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 20, 3, 60)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 20, 3, 64)    46144       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 3, 64)    256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 20, 3, 64)    4160        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 3, 64)    3904        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 3, 64)    256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 3, 64)    256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 20, 3, 64)    0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 20, 3, 64)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 20, 3, 64)    49216       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 3, 64)    256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 3, 64)    4160        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 3, 64)    256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 20, 3, 64)    0           re_lu_3[0][0]                    \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 20, 3, 64)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 3, 64)    49216       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 3, 64)    256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 20, 3, 64)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 20, 3, 64)    4160        re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 3, 64)    256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 20, 3, 64)    0           re_lu_6[0][0]                    \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 20, 3, 64)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 20, 3, 128)   98432       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 3, 128)   512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 3, 128)   8320        re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 3, 128)   512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 3, 128)   512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 20, 3, 128)   0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 20, 3, 128)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 3, 128)   512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 3, 128)   512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 20, 3, 128)   0           re_lu_12[0][0]                   \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 20, 3, 128)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 3, 128)   512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 3, 128)   512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 20, 3, 128)   0           re_lu_15[0][0]                   \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 20, 3, 128)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 3, 128)   512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 3, 128)   512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 20, 3, 128)   0           re_lu_18[0][0]                   \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 20, 3, 128)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 20, 3, 256)   393472      re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 3, 256)   1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 20, 3, 256)   33024       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 3, 256)   1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 3, 256)   1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 20, 3, 256)   0           batch_normalization_26[0][0]     \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 20, 3, 256)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 3, 256)   1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 3, 256)   1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 20, 3, 256)   0           re_lu_24[0][0]                   \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 20, 3, 256)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 3, 256)   1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 3, 256)   1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 20, 3, 256)   0           re_lu_27[0][0]                   \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 20, 3, 256)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 3, 256)   1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 3, 256)   1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 20, 3, 256)   0           re_lu_30[0][0]                   \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 20, 3, 256)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 3, 256)   1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 3, 256)   1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 20, 3, 256)   0           re_lu_33[0][0]                   \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 20, 3, 256)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 20, 3, 256)   1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 20, 3, 256)   1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 20, 3, 256)   0           re_lu_36[0][0]                   \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 20, 3, 256)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 20, 3, 512)   1573376     re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 20, 3, 512)   2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 20, 3, 512)   131584      re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 20, 3, 512)   2048        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 20, 3, 512)   2048        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 20, 3, 512)   0           batch_normalization_45[0][0]     \n",
      "                                                                 batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 20, 3, 512)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 20, 3, 512)   3146240     re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 20, 3, 512)   2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 20, 3, 512)   2048        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20, 3, 512)   0           re_lu_42[0][0]                   \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 20, 3, 512)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 20, 3, 512)   3146240     re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 20, 3, 512)   2048        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 20, 3, 512)   2048        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 3, 512)   0           re_lu_45[0][0]                   \n",
      "                                                                 batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 20, 3, 512)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 10, 1, 512)   0           re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 5120)         0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            10242       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 24s - loss: 1.3863 - accuracy: 0.7477 - val_loss: 7.4150 - val_accuracy: 0.5059\n",
      "Epoch 2/100\n",
      "24/24 - 22s - loss: 0.7480 - accuracy: 0.8549 - val_loss: 1.8796 - val_accuracy: 0.7294\n",
      "Epoch 3/100\n",
      "24/24 - 25s - loss: 0.6702 - accuracy: 0.8771 - val_loss: 1.6090 - val_accuracy: 0.8706\n",
      "Epoch 4/100\n",
      "24/24 - 25s - loss: 0.4251 - accuracy: 0.8863 - val_loss: 0.9194 - val_accuracy: 0.8824\n",
      "Epoch 5/100\n",
      "24/24 - 25s - loss: 0.3801 - accuracy: 0.8993 - val_loss: 0.2989 - val_accuracy: 0.8706\n",
      "Epoch 6/100\n",
      "24/24 - 27s - loss: 0.3293 - accuracy: 0.9007 - val_loss: 0.1884 - val_accuracy: 0.9176\n",
      "Epoch 7/100\n",
      "24/24 - 26s - loss: 0.3110 - accuracy: 0.9072 - val_loss: 0.4559 - val_accuracy: 0.8824\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t93.88889%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       246\n",
      "           1       0.96      0.84      0.90       114\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.95      0.91      0.93       360\n",
      "weighted avg       0.94      0.94      0.94       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
