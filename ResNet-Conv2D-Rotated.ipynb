{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 1, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling1D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,1) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,1) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,1,3])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D(pool_size=(2,1))(X)\n",
    "#     X = AveragePooling1D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 1, 60)\n",
      "(None, 20, 1, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 20, 1, 3)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_530 (Conv2D)             (None, 20, 1, 60)    240         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_530 (BatchN (None, 20, 1, 60)    240         conv2d_530[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_490 (ReLU)                (None, 20, 1, 60)    0           batch_normalization_530[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_532 (Conv2D)             (None, 20, 1, 64)    15424       re_lu_490[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_532 (BatchN (None, 20, 1, 64)    256         conv2d_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_492 (ReLU)                (None, 20, 1, 64)    0           batch_normalization_532[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_533 (Conv2D)             (None, 20, 1, 64)    4160        re_lu_492[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_534 (Conv2D)             (None, 20, 1, 64)    3904        re_lu_490[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_533 (BatchN (None, 20, 1, 64)    256         conv2d_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_534 (BatchN (None, 20, 1, 64)    256         conv2d_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_160 (Add)                   (None, 20, 1, 64)    0           batch_normalization_533[0][0]    \n",
      "                                                                 batch_normalization_534[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_493 (ReLU)                (None, 20, 1, 64)    0           add_160[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 20, 1, 64)    16448       re_lu_493[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_536 (BatchN (None, 20, 1, 64)    256         conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_495 (ReLU)                (None, 20, 1, 64)    0           batch_normalization_536[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 20, 1, 64)    4160        re_lu_495[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_537 (BatchN (None, 20, 1, 64)    256         conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_161 (Add)                   (None, 20, 1, 64)    0           re_lu_493[0][0]                  \n",
      "                                                                 batch_normalization_537[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_496 (ReLU)                (None, 20, 1, 64)    0           add_161[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 20, 1, 64)    16448       re_lu_496[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_539 (BatchN (None, 20, 1, 64)    256         conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_498 (ReLU)                (None, 20, 1, 64)    0           batch_normalization_539[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 20, 1, 64)    4160        re_lu_498[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_540 (BatchN (None, 20, 1, 64)    256         conv2d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_162 (Add)                   (None, 20, 1, 64)    0           re_lu_496[0][0]                  \n",
      "                                                                 batch_normalization_540[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_499 (ReLU)                (None, 20, 1, 64)    0           add_162[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 20, 1, 128)   32896       re_lu_499[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_542 (BatchN (None, 20, 1, 128)   512         conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_501 (ReLU)                (None, 20, 1, 128)   0           batch_normalization_542[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 20, 1, 128)   16512       re_lu_501[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 20, 1, 128)   8320        re_lu_499[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_543 (BatchN (None, 20, 1, 128)   512         conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_544 (BatchN (None, 20, 1, 128)   512         conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_163 (Add)                   (None, 20, 1, 128)   0           batch_normalization_543[0][0]    \n",
      "                                                                 batch_normalization_544[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_502 (ReLU)                (None, 20, 1, 128)   0           add_163[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 20, 1, 128)   65664       re_lu_502[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_546 (BatchN (None, 20, 1, 128)   512         conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_504 (ReLU)                (None, 20, 1, 128)   0           batch_normalization_546[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 20, 1, 128)   16512       re_lu_504[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_547 (BatchN (None, 20, 1, 128)   512         conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_164 (Add)                   (None, 20, 1, 128)   0           re_lu_502[0][0]                  \n",
      "                                                                 batch_normalization_547[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_505 (ReLU)                (None, 20, 1, 128)   0           add_164[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_549 (Conv2D)             (None, 20, 1, 128)   65664       re_lu_505[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_549 (BatchN (None, 20, 1, 128)   512         conv2d_549[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_507 (ReLU)                (None, 20, 1, 128)   0           batch_normalization_549[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_550 (Conv2D)             (None, 20, 1, 128)   16512       re_lu_507[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_550 (BatchN (None, 20, 1, 128)   512         conv2d_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_165 (Add)                   (None, 20, 1, 128)   0           re_lu_505[0][0]                  \n",
      "                                                                 batch_normalization_550[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_508 (ReLU)                (None, 20, 1, 128)   0           add_165[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_552 (Conv2D)             (None, 20, 1, 128)   65664       re_lu_508[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_552 (BatchN (None, 20, 1, 128)   512         conv2d_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_510 (ReLU)                (None, 20, 1, 128)   0           batch_normalization_552[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_553 (Conv2D)             (None, 20, 1, 128)   16512       re_lu_510[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_553 (BatchN (None, 20, 1, 128)   512         conv2d_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_166 (Add)                   (None, 20, 1, 128)   0           re_lu_508[0][0]                  \n",
      "                                                                 batch_normalization_553[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_511 (ReLU)                (None, 20, 1, 128)   0           add_166[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_555 (Conv2D)             (None, 20, 1, 256)   131328      re_lu_511[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_555 (BatchN (None, 20, 1, 256)   1024        conv2d_555[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_513 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_555[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_556 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_513[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_557 (Conv2D)             (None, 20, 1, 256)   33024       re_lu_511[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_556 (BatchN (None, 20, 1, 256)   1024        conv2d_556[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_557 (BatchN (None, 20, 1, 256)   1024        conv2d_557[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_167 (Add)                   (None, 20, 1, 256)   0           batch_normalization_556[0][0]    \n",
      "                                                                 batch_normalization_557[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_514 (ReLU)                (None, 20, 1, 256)   0           add_167[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_559 (Conv2D)             (None, 20, 1, 256)   262400      re_lu_514[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_559 (BatchN (None, 20, 1, 256)   1024        conv2d_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_516 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_559[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_560 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_516[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_560 (BatchN (None, 20, 1, 256)   1024        conv2d_560[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_168 (Add)                   (None, 20, 1, 256)   0           re_lu_514[0][0]                  \n",
      "                                                                 batch_normalization_560[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_517 (ReLU)                (None, 20, 1, 256)   0           add_168[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_562 (Conv2D)             (None, 20, 1, 256)   262400      re_lu_517[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_562 (BatchN (None, 20, 1, 256)   1024        conv2d_562[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_519 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_562[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_563 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_519[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_563 (BatchN (None, 20, 1, 256)   1024        conv2d_563[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_169 (Add)                   (None, 20, 1, 256)   0           re_lu_517[0][0]                  \n",
      "                                                                 batch_normalization_563[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_520 (ReLU)                (None, 20, 1, 256)   0           add_169[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_565 (Conv2D)             (None, 20, 1, 256)   262400      re_lu_520[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_565 (BatchN (None, 20, 1, 256)   1024        conv2d_565[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_522 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_565[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_566 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_522[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_566 (BatchN (None, 20, 1, 256)   1024        conv2d_566[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_170 (Add)                   (None, 20, 1, 256)   0           re_lu_520[0][0]                  \n",
      "                                                                 batch_normalization_566[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_523 (ReLU)                (None, 20, 1, 256)   0           add_170[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_568 (Conv2D)             (None, 20, 1, 256)   262400      re_lu_523[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_568 (BatchN (None, 20, 1, 256)   1024        conv2d_568[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_525 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_568[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_569 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_525[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_569 (BatchN (None, 20, 1, 256)   1024        conv2d_569[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_171 (Add)                   (None, 20, 1, 256)   0           re_lu_523[0][0]                  \n",
      "                                                                 batch_normalization_569[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_526 (ReLU)                (None, 20, 1, 256)   0           add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_571 (Conv2D)             (None, 20, 1, 256)   262400      re_lu_526[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_571 (BatchN (None, 20, 1, 256)   1024        conv2d_571[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_528 (ReLU)                (None, 20, 1, 256)   0           batch_normalization_571[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_572 (Conv2D)             (None, 20, 1, 256)   65792       re_lu_528[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_572 (BatchN (None, 20, 1, 256)   1024        conv2d_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_172 (Add)                   (None, 20, 1, 256)   0           re_lu_526[0][0]                  \n",
      "                                                                 batch_normalization_572[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_529 (ReLU)                (None, 20, 1, 256)   0           add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_574 (Conv2D)             (None, 20, 1, 512)   524800      re_lu_529[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_574 (BatchN (None, 20, 1, 512)   2048        conv2d_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_531 (ReLU)                (None, 20, 1, 512)   0           batch_normalization_574[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_575 (Conv2D)             (None, 20, 1, 512)   262656      re_lu_531[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_576 (Conv2D)             (None, 20, 1, 512)   131584      re_lu_529[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_575 (BatchN (None, 20, 1, 512)   2048        conv2d_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_576 (BatchN (None, 20, 1, 512)   2048        conv2d_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_173 (Add)                   (None, 20, 1, 512)   0           batch_normalization_575[0][0]    \n",
      "                                                                 batch_normalization_576[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_532 (ReLU)                (None, 20, 1, 512)   0           add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_578 (Conv2D)             (None, 20, 1, 512)   1049088     re_lu_532[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 20, 1, 512)   2048        conv2d_578[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_534 (ReLU)                (None, 20, 1, 512)   0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_579 (Conv2D)             (None, 20, 1, 512)   262656      re_lu_534[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 20, 1, 512)   2048        conv2d_579[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_174 (Add)                   (None, 20, 1, 512)   0           re_lu_532[0][0]                  \n",
      "                                                                 batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_535 (ReLU)                (None, 20, 1, 512)   0           add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_581 (Conv2D)             (None, 20, 1, 512)   1049088     re_lu_535[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 20, 1, 512)   2048        conv2d_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_537 (ReLU)                (None, 20, 1, 512)   0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_582 (Conv2D)             (None, 20, 1, 512)   262656      re_lu_537[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 20, 1, 512)   2048        conv2d_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_175 (Add)                   (None, 20, 1, 512)   0           re_lu_535[0][0]                  \n",
      "                                                                 batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_538 (ReLU)                (None, 20, 1, 512)   0           add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 10, 1, 512)   0           re_lu_538[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 5120)         0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            10242       flatten_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,827,362\n",
      "Trainable params: 5,810,218\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 8s - loss: 1.1810 - accuracy: 0.8052 - val_loss: 8.1196 - val_accuracy: 0.4706\n",
      "Epoch 2/100\n",
      "24/24 - 7s - loss: 0.5567 - accuracy: 0.9020 - val_loss: 4.3729 - val_accuracy: 0.7059\n",
      "Epoch 3/100\n",
      "24/24 - 7s - loss: 0.5046 - accuracy: 0.8967 - val_loss: 1.6160 - val_accuracy: 0.8706\n",
      "Epoch 4/100\n",
      "24/24 - 8s - loss: 0.4383 - accuracy: 0.9150 - val_loss: 0.5651 - val_accuracy: 0.9059\n",
      "Epoch 5/100\n",
      "24/24 - 9s - loss: 0.2638 - accuracy: 0.9438 - val_loss: 0.6019 - val_accuracy: 0.7765\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t81.38889%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       246\n",
      "           1       0.98      0.42      0.59       114\n",
      "\n",
      "    accuracy                           0.81       360\n",
      "   macro avg       0.88      0.71      0.73       360\n",
      "weighted avg       0.85      0.81      0.79       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
