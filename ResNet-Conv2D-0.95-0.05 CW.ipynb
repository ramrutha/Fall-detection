{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 20, 3, 60)    120         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 20, 3, 60)    240         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, 20, 3, 60)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 20, 3, 64)    46144       re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 20, 3, 64)    256         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, 20, 3, 64)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 20, 3, 64)    4160        re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 20, 3, 64)    3904        re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 20, 3, 64)    256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 20, 3, 64)    256         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 20, 3, 64)    0           batch_normalization_56[0][0]     \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 20, 3, 64)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 20, 3, 64)    49216       re_lu_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 20, 3, 64)    256         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, 20, 3, 64)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 20, 3, 64)    4160        re_lu_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 20, 3, 64)    256         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 20, 3, 64)    0           re_lu_52[0][0]                   \n",
      "                                                                 batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 20, 3, 64)    0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 20, 3, 64)    49216       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 20, 3, 64)    256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, 20, 3, 64)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 20, 3, 64)    4160        re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 20, 3, 64)    256         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 20, 3, 64)    0           re_lu_55[0][0]                   \n",
      "                                                                 batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, 20, 3, 64)    0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 20, 3, 128)   98432       re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 20, 3, 128)   512         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 20, 3, 128)   8320        re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 20, 3, 128)   512         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 20, 3, 128)   512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 20, 3, 128)   0           batch_normalization_66[0][0]     \n",
      "                                                                 batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_61 (ReLU)                 (None, 20, 3, 128)   0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 20, 3, 128)   512         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_63 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 20, 3, 128)   512         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 20, 3, 128)   0           re_lu_61[0][0]                   \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, 20, 3, 128)   0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 20, 3, 128)   512         conv2d_72[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 20, 3, 128)   512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 20, 3, 128)   0           re_lu_64[0][0]                   \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, 20, 3, 128)   0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 20, 3, 128)   196736      re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 20, 3, 128)   512         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_69 (ReLU)                 (None, 20, 3, 128)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 20, 3, 128)   16512       re_lu_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 20, 3, 128)   512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 20, 3, 128)   0           re_lu_67[0][0]                   \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_70 (ReLU)                 (None, 20, 3, 128)   0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 20, 3, 256)   393472      re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 20, 3, 256)   1024        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_72 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 20, 3, 256)   33024       re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 20, 3, 256)   1024        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 20, 3, 256)   1024        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 20, 3, 256)   0           batch_normalization_79[0][0]     \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_73 (ReLU)                 (None, 20, 3, 256)   0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 20, 3, 256)   1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_75 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 20, 3, 256)   1024        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 20, 3, 256)   0           re_lu_73[0][0]                   \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_76 (ReLU)                 (None, 20, 3, 256)   0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 20, 3, 256)   1024        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_78 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 20, 3, 256)   1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 20, 3, 256)   0           re_lu_76[0][0]                   \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_79 (ReLU)                 (None, 20, 3, 256)   0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 20, 3, 256)   1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_81 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 20, 3, 256)   1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 20, 3, 256)   0           re_lu_79[0][0]                   \n",
      "                                                                 batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_82 (ReLU)                 (None, 20, 3, 256)   0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 20, 3, 256)   1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_84 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 20, 3, 256)   1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 20, 3, 256)   0           re_lu_82[0][0]                   \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_85 (ReLU)                 (None, 20, 3, 256)   0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 20, 3, 256)   786688      re_lu_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 20, 3, 256)   1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_87 (ReLU)                 (None, 20, 3, 256)   0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 20, 3, 256)   65792       re_lu_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 20, 3, 256)   1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 20, 3, 256)   0           re_lu_85[0][0]                   \n",
      "                                                                 batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_88 (ReLU)                 (None, 20, 3, 256)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 20, 3, 512)   1573376     re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 20, 3, 512)   2048        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_90 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 20, 3, 512)   262656      re_lu_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 20, 3, 512)   131584      re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 20, 3, 512)   2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 20, 3, 512)   2048        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 20, 3, 512)   0           batch_normalization_98[0][0]     \n",
      "                                                                 batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_91 (ReLU)                 (None, 20, 3, 512)   0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 20, 3, 512)   2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_93 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 20, 3, 512)   2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 20, 3, 512)   0           re_lu_91[0][0]                   \n",
      "                                                                 batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_94 (ReLU)                 (None, 20, 3, 512)   0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 20, 3, 512)   2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_96 (ReLU)                 (None, 20, 3, 512)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 20, 3, 512)   2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 20, 3, 512)   0           re_lu_94[0][0]                   \n",
      "                                                                 batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_97 (ReLU)                 (None, 20, 3, 512)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 10, 1, 512)   0           re_lu_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5120)         0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            10242       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 29s - loss: 1.1869 - accuracy: 0.6889 - val_loss: 8.1196 - val_accuracy: 0.4706\n",
      "Epoch 2/100\n",
      "24/24 - 29s - loss: 0.6639 - accuracy: 0.8627 - val_loss: 1.6747 - val_accuracy: 0.6353\n",
      "Epoch 3/100\n",
      "24/24 - 33s - loss: 0.3484 - accuracy: 0.8824 - val_loss: 0.2932 - val_accuracy: 0.8941\n",
      "Epoch 4/100\n",
      "24/24 - 47s - loss: 0.3758 - accuracy: 0.8824 - val_loss: 0.8665 - val_accuracy: 0.6353\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "class_weight = {0: 0.95,\n",
    "                1: 0.05}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t63.88889%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.52      0.66       246\n",
      "           1       0.46      0.89      0.61       114\n",
      "\n",
      "    accuracy                           0.64       360\n",
      "   macro avg       0.69      0.71      0.64       360\n",
      "weighted avg       0.77      0.64      0.65       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXrUlEQVR4nO3dd5xdZb3v8c8vM8Q0ivReJEoJaOgiFlCKAQG5yBEOHEUpEbscFRQQIxwuVwH1XJQWFEENTUC44AFFpR8CF2kJRlqAQIDQY2gpv/PHWgM7ccoGZ88m83zer9d+Za1ntd/smXz3s561Zk1kJpKkwW9IuwuQJA0MA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGvtSHqPw8Ip6NiMn/xH4+EBHT+rO2doiIUyPiqHbXoTfOwNc/iIjpEfFSRPw9Ih6PiLMiYlQb69k/Iq5vYr2dIuLaiJgdEbMi4pqI2K0fSng/sAOwemZu+WZ3kpnXZeZ6/VDPQiJi7YjIiLhtkfblI+LViJje5H6aep8z83OZecybLFdtZOCrJ7tm5ihgLLAJ8K0219OriPgEcAFwNrA6sBLwHWDXftj9WsD0zJzTD/tqpZERsVHD/L8CD/bnASKioz/3p4Fl4KtXmfk4cCVV8AMQEe+NiBsj4rmIuCMitm1Ytn9EPFD3sh+MiH0b2q+PiBPqoZEHI2Jcw3ZLR8SZETEzIh6NiGMjoiMiNgBOBbauzzieW7TGiAjgJOCYzJyYmc9n5oLMvCYzD6rXGRIRR0bEQxHxZEScHRFL18u6esifjoiHI+KpiDiiXnYAMLHh+BO66wnX24+up3eOiKn1e/BoRHy9bt82ImY0bLNBRPy5fh+nNJ6N1GdVP4mIy+v93BwR6/bx7ToH+HTD/KeoPgAb6zw8Iu6v9zk1IvboqqW797mu45SIuCIi5gDb1W3H1ssPi4j/jojOev6Q+msZ1ketaofM9OVroRcwHdi+nl4duAv4cT2/GvA0sDNVh2GHen4FYCTwArBeve4qwJh6en9gLnAQ0AEcAjwGRL38EuC0eh8rApOB8Q3bXt9LvesDCazTyzqfBe4D3gGMAi4CzqmXrV1vfwYwHHgP8AqwQXfH766eevvR9fRM4AP19NuBTevpbYEZ9fQSdT3fBoYCHwZmN7x3ZwHPAFsCncCvgHN7+Nq66l8beKR+fzcApgHbU52ddK27F7Bq/b37JDAHWKWXr+ss4Hlgm3qbYXXbsfXyIcC1wHeBdwLPApu0+2fYV/cve/jqySURMZsqQJ4Ejq7b9wOuyMwrsupF/x64leoDAGABsFFEDM/MmZk5pWGfD2XmGZk5H/gF1QfCShGxEjAO+GpmzsnMJ4EfAns3Wety9b8ze1lnX+CkzHwgM/9ONUS1d1fPtDYhM1/KzDuAO6iC/82YC2wYEUtl5rOZeVs367yX6oPn+Mx8NTP/CPw/YJ+GdS7KzMmZOY8q8Md2s59GM3g95D/NIr17gMy8IDMfq7935wH3Un2o9Oa3mXlDvc3Li+xvAdWZxJeBS4HvZ+Zf+tif2sTAV08+nplLUvVK1weWr9vXAvaqhyGeq0/930/VS5xD1Wv8HDCzHo5Yv2Gfj3dNZOaL9eSoep9L1Nt07fM0qp5+M56u/12ll3VWBR5qmH+Ique8Unf1AS/Wtb0Ze1J9AD5UXzjeuod6HqkDs7Gm1f7Jes6m6qnvA/xy0YUR8amIuL3hfd6I17+3PXmkt4WZOR34E9UZxk+aqFFtYuCrV5l5DdUp/Al10yNUQyHLNLxGZubx9fpXZuYOVOH7V6phkr48QjWEsnzDPpfKzDFdZfSx/bR6H3v2ss5jVB8sXdYE5gFPNFHfouYAI7pmImLlxoWZeUtm7k71gXUJcH4P9awREY3/B9cEHn0T9TT6DbAL8EBmNn7AERFrUX0/vggsl5nLAHcD0VV6D/vs9f2PiJ2BrYGrgR+8+dLVaga+mvEjYIeIGEvVa9w1qlsgOyJiWH0xcvWIWCkidouIkVQB/ndgfl87z8yZwFXAiRGxVH2Bdd2I+FC9yhPA6hExtIftEzgUOCoiPtOwj/dHxOn1apOAr0XEOlHdYnoccF49XPJG3QGMiYix9cXJ73YtiIihEbFvRCydmXOprml09x7cTPXB8c2IWCKqC9+7Aue+iXpeU59lfRg4sJvFI6nCe1Zd62eoevhden2fuxMRywNn1sf7NNXPxs69b6V2MfDVp8ycRTVUcFRmPgLsTnWxcRZVz/obVD9LQ4B/p+q9PgN8CPh8k4f5FNXFy6lUF/4u5PUhmj8CU4DHI+KpHmq8kGo46bP18Z8AjgV+W6/yM6q7WK6lulXxZeBLTda26LH+BnwP+APVGPii967/GzA9Il6gGt7ar5t9vArsRnXt4ingp8CnMvOvb6amRfZ9a2be3037VOBE4Caq92dj4IaGVfp8n7txOtUY/xWZ+TRwADAxIpbrYzu1QdcdEpKkQc4eviQVwsCXpEIY+JJUCANfkgrR2fcq7TH3qQe8mqy3pPnTbmp3CVKPhm2zb/S0zB6+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQnS2uwD984487iSuvWEyy759GS755akAnHDyRK654WY6l+hkjdVW4dhvH8pSS45i7ty5TPj+/2XKX+8lhgSHf+VzbLnpu9v8FWgw+87PLuXaO/7GskuN5KJjDgHgqlumcspvr+HBmbP41ZEHMmadVQG4/Ka7+MV/3fjatn+b8QTnHn0w66+5cltqH2zs4Q8CH995B0496diF2rbeYhMuPudULj77FNZeYzUmnnMeABde+l8AXHzOKZzxo+M44eQzWLBgwYDXrHLsvs17OOXQfRdqG73aCvzwC3ux2bvWWqh9l6035vwJ4zl/wnj+46CPs+pyyxj2/ahlPfyIWB/YHVgNSOAx4NLMvKdVxyzV5mM35tGZTyzUts1Wm702/e4x6/P7P10PwP3TH2arzccCsNzbl2HJUSOZ8td72XjD9QauYBVls/XW4tGnnluo7R2rrtDndr+7+W7GbbVRq8oqUkt6+BFxGHAuEMBk4JZ6elJEHN6KY6pnF19+Fe/fegsA1hu9Dn+67ibmzZvPjMceZ+q0+3j8iVltrlD6R1dOnspHDfx+1aoe/gHAmMyc29gYEScBU4Dju9soIg4GDgb46YnHcuCn9mlReeU47ReT6Ojo4GM7bgfAHrvsxAPTH+GTB3yZVVdekbEbbUBHZ0ebq5QWduf9Mxg2dAneufqK7S5lUGlV4C8AVgUeWqR9lXpZtzLzdOB0gLlPPZAtqq0Yv73i91x7w2Qm/uf/JiIA6Ozs4LCvjH9tnX3HH8paq6/arhKlbl05eQrjthrT7jIGnVYF/leBqyPiXuCRum1NYDTwxRYdUw2u/+9bOfNXF3DWyd9n+LBhr7W/9PLLZMKI4cO4cfJtdHZ0sO46a/WyJ2lgLViQXHXrVH5+2P7tLmXQiczWdKQjYgiwJdVF2wBmALdk5vxmtreH37xvHH08t/zlTp577gWWW3YZPn/AvzHxnPN4de5clllqKaC6cHv0N7/EozOfYPzXjiCGDGGlFZbje9/6KquuvFKbv4LFy/xpN7W7hMXKYaf+hlunPcRzf3+RZZcaySG7b8vSI4dz/K9/x7OzX2TJEcNYb42VOPXf9wPglr9O58cXXs0vjzygzZUvnoZts2/0tKxlgf/PMvD1VmXg662st8D3PnxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKkRTgR8Ra0XE9vX08IhYsrVlSZL6W5+BHxEHARcCp9VNqwOXtLIoSVL/a6aH/wVgG+AFgMy8F1ixlUVJkvpfM4H/Sma+2jUTEZ1Atq4kSVIrNBP410TEt4HhEbEDcAFwWWvLkiT1t2YC/3BgFnAXMB64AjiylUVJkvpfZ18rZOYC4Iz6JUlaTPUZ+BHxIN2M2WfmO1pSkSSpJfoMfGDzhulhwF7Asq0pR5LUKn2O4Wfm0w2vRzPzR8CHB6A2SVI/amZIZ9OG2SFUPX5/01aSFjPNDOmc2DA9D5gO/EtLqpEktUwzd+lsNxCFSJJaq8fAj4hDe9swM0/q/3IkSa3SWw/fcXpJGkR6DPzMnDCQhUiSWquZu3SGAQcAY6juwwcgMz/bwrokSf2smWfpnAOsDOwEXEP1PPzZrSxKktT/mgn80Zl5FDAnM38B7AJs3NqyJEn9rZnAn1v/+1xEbAQsDazdsookSS3RzC9enR4RbweOAi4FRtXTkqTFSG/34U8FfgWcm5nPUo3f+4RMSVpM9Taksw9Vb/6qiLg5Ir4aEasMUF2SpH7WY+Bn5h2Z+a3MXBf4CrAWcHNE/DEiDhqwCiVJ/SIym/975BGxLfBDYMPMfFurigLoHLqafyhdb0kvHDeu3SVIPRrx9YnR07JmfvFqC6rhnT2pnpR5OtUfMpckLUZ6u2h7HPBJ4FngXGCbzJwxUIVJkvpXbz38V4Bxmfm3gSpGktQ6PjxNkgrRzG/aSpIGAQNfkgrRZ+BHZb+I+E49v2ZEbNn60iRJ/amZHv5Pga2pbs2E6tHIP2lZRZKklmjm4WlbZeamEfEXgMx8NiKGtrguSVI/a+rxyBHRASRARKwALGhpVZKkftdM4P8ncDGwYkT8B3A9cFxLq5Ik9bs+h3Qy81cR8f+BjwABfDwz72l5ZZKkftXMs3TWBF4ELmtsy8yHW1mYJKl/NXPR9nKq8fsAhgHrANOAMS2sS5LUz5oZ0lnoD5ZHxKbA+JZVJElqiTf8m7aZeRuwRQtqkSS1UDNj+Ic2zA4BNgVmtawiSVJLNDOGv2TD9DyqMf3ftKYcSVKr9Br49S9cjcrMbwxQPZKkFulxDD8iOjNzPtUQjiRpMddbD38yVdjfHhGXUv0d2zldCzPzohbXJknqR82M4S8LPA18mNfvx0/AwJekxUhvgb9ifYfO3bwe9F2ypVVJkvpdb4HfAYxi4aDvYuBL0mKmt8CfmZnfG7BKJEkt1dtv2nbXs5ckLaZ6C/yPDFgVkqSW6zHwM/OZgSxEktRab/jhaZKkxZOBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQnS2uwD1vzNOP5Fddt6eJ2c9xdhNPgLAhO9+g1133ZEFC5JZTz7FZw/8GjNnPtHmSlWCoTvtT8e67yZfnM3LZx1dNQ4byds+Np5Yejny+ad55bJT4ZUXq/bdDmHIymszb8qNzL361+0tfpCxhz8InX32+ezysX0XajvhxFPYdLMd2HyLHbn8ij9w5BFfa1N1Ks28KTfw8oU/WqhtiS3HMf/he3j5zCOY//A9LLHVuGrB/LnMveES5l5zQRsqHfwM/EHouutv5plnn1uobfbsv782PXLkCDJzoMtSoRbMuBdenrNQW8foscybciMA86bcSMfoTaoFc19lwaP3kfPmDnSZRXBIpyDHfO8w9tv3Ezz/wgtsv8Ne7S5HBYsRS8Gc56uZOc8TI5Zsb0GFGPAefkR8ppdlB0fErRFx64IFc3paTW/SUd/5P6yz7hZMmnQxX/h8j98GSYNUO4Z0JvS0IDNPz8zNM3PzIUNGDmRNRZl07sXsscfO7S5DBcsXX4CRS1czI5cmX5zd3oIK0ZIhnYi4s6dFwEqtOKZ6N3r0Otx334MA7PqxHZk27f42V6SSzb//djrHvI95k39H55j3Mf++29tdUhFaNYa/ErAT8Owi7QHc2KJjqvbLc37Chz64NcsvvyzTH7iVCd87gXHjPsy73rUuCxYs4OGHH+XzXzi83WWqEEN3OYiONdaD4aMYNv77zL3hUube/Dvetuvn6Nz4/eQLz1S3ZdaGHXQ8MXQ4dHTQMXosr1z4Q/LpmW38CgaPaMXdGhFxJvDzzLy+m2W/zsx/7WsfnUNX8zYSvSW9cNy4dpcg9WjE1ydGT8ta0sPPzAN6WdZn2EuS+p/34UtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSISIz212DBkBEHJyZp7e7DmlR/mwOHHv45Ti43QVIPfBnc4AY+JJUCANfkgph4JfDMVK9VfmzOUC8aCtJhbCHL0mFMPAlqRAG/iAXER+NiGkRcV9EHN7ueqQuEfGziHgyIu5udy2lMPAHsYjoAH4CjAM2BPaJiA3bW5X0mrOAj7a7iJIY+IPblsB9mflAZr4KnAvs3uaaJAAy81rgmXbXURIDf3BbDXikYX5G3SapQAb+4BbdtHkfrlQoA39wmwGs0TC/OvBYm2qR1GYG/uB2C/DOiFgnIoYCewOXtrkmSW1i4A9imTkP+CJwJXAPcH5mTmlvVVIlIiYBNwHrRcSMiDig3TUNdj5aQZIKYQ9fkgph4EtSIQx8SSqEgS9JhTDwJakQBr7e0iJifkTcHhF3R8QFETHin9jXWRHxiXp6Ym8PkouIbSPifW/iGNMjYvlujjt+kbaPR8QVzdQq9RcDX291L2Xm2MzcCHgV+FzjwvqJoG9YZh6YmVN7WWVb4A0Hfg8mUf3SW6O963ZpwBj4WpxcB4yue99/iohfA3dFREdE/CAibomIO7t601E5OSKmRsTlwIpdO4qIP0fE5vX0RyPitoi4IyKujoi1qT5YvlafXXwgIlaIiN/Ux7glIrapt10uIq6KiL9ExGl0//yiPwDrR8Qq9TYjgO2BSyLiO/X+7o6I0yPiH7ZvPGuIiM0j4s/19Mj6mfK31Mf3SajqlYGvxUJEdFI91/+uumlL4IjM3BA4AHg+M7cAtgAOioh1gD2A9YCNgYPopsceESsAZwB7ZuZ7gL0yczpwKvDD+uziOuDH9fwWwJ7AxHoXRwPXZ+YmVI+tWHPRY2TmfOAi4F/qpt2AP2XmbODkzNyiPoMZDnzsDbwtRwB/rGvaDvhBRIx8A9urMJ3tLkDqw/CIuL2evg44kyq4J2fmg3X7jsC7G8a8lwbeCXwQmFQH7mMR8cdu9v9e4NqufWVmT89n3x7YsKEDvlRELFkf43/V214eEc/2sP0k4AdUHxx7A2fX7dtFxDeBEcCywBTgsh72sagdgd0i4uv1/DCqD5x7mtxehTHw9Vb3UmaObWyoQ3dOYxPwpcy8cpH1dqbvx0FHE+tAdTa8dWa+1E0tzWx/A7BKRLyH6gNr74gYBvwU2DwzH4mI71KF9qLm8frZeOPyoDozmdbE8SWHdDQoXAkcEhFLAETEu+qhjWupgrWjHj/frpttbwI+VA8BERHL1u2zgSUb1ruK6kF01Ot1fQhdC+xbt40D3t5dgVk9tOp84BfAFZn5Mq+H91MRMQro6a6c6cBm9fSei3zdX+oa94+ITXrYXgIMfA0OE4GpwG31H8Q+jers9WLgXqpx/1OAaxbdMDNnAQcDF0XEHcB59aLLgD26LtoCXwY2ry8KT+X1u4UmAB+MiNuohlge7qXOScB7qP7UJJn5HNX1g7uAS6geZ92dCcCPI+I6YH5D+zHAEsCd9dd9TC/HlnxapiSVwh6+JBXCwJekQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mF+B/yt2LWxth1/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
