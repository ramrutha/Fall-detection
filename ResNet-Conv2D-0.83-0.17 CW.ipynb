{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 20, 3, 60)    120         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 20, 3, 60)    240         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_98 (ReLU)                 (None, 20, 3, 60)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 20, 3, 64)    46144       re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 20, 3, 64)    256         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_100 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 20, 3, 64)    3904        re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 20, 3, 64)    256         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 20, 3, 64)    256         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 20, 3, 64)    0           batch_normalization_109[0][0]    \n",
      "                                                                 batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_101 (ReLU)                (None, 20, 3, 64)    0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 20, 3, 64)    256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_103 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 20, 3, 64)    256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 20, 3, 64)    0           re_lu_101[0][0]                  \n",
      "                                                                 batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_104 (ReLU)                (None, 20, 3, 64)    0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 20, 3, 64)    256         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_106 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 20, 3, 64)    256         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 20, 3, 64)    0           re_lu_104[0][0]                  \n",
      "                                                                 batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_107 (ReLU)                (None, 20, 3, 64)    0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 20, 3, 128)   98432       re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 20, 3, 128)   512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_109 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 20, 3, 128)   8320        re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 20, 3, 128)   512         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 20, 3, 128)   512         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 20, 3, 128)   0           batch_normalization_119[0][0]    \n",
      "                                                                 batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_110 (ReLU)                (None, 20, 3, 128)   0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 20, 3, 128)   512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_112 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 20, 3, 128)   512         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 20, 3, 128)   0           re_lu_110[0][0]                  \n",
      "                                                                 batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_113 (ReLU)                (None, 20, 3, 128)   0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 20, 3, 128)   512         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_115 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 20, 3, 128)   512         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 20, 3, 128)   0           re_lu_113[0][0]                  \n",
      "                                                                 batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_116 (ReLU)                (None, 20, 3, 128)   0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 20, 3, 128)   512         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_118 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 20, 3, 128)   512         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 20, 3, 128)   0           re_lu_116[0][0]                  \n",
      "                                                                 batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_119 (ReLU)                (None, 20, 3, 128)   0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 20, 3, 256)   393472      re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 20, 3, 256)   1024        conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_121 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 20, 3, 256)   33024       re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 20, 3, 256)   1024        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 20, 3, 256)   1024        conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 20, 3, 256)   0           batch_normalization_132[0][0]    \n",
      "                                                                 batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_122 (ReLU)                (None, 20, 3, 256)   0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 20, 3, 256)   1024        conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_124 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 20, 3, 256)   1024        conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 20, 3, 256)   0           re_lu_122[0][0]                  \n",
      "                                                                 batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_125 (ReLU)                (None, 20, 3, 256)   0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 20, 3, 256)   1024        conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_127 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 20, 3, 256)   1024        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 20, 3, 256)   0           re_lu_125[0][0]                  \n",
      "                                                                 batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_128 (ReLU)                (None, 20, 3, 256)   0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 20, 3, 256)   1024        conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_130 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 20, 3, 256)   1024        conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 20, 3, 256)   0           re_lu_128[0][0]                  \n",
      "                                                                 batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_131 (ReLU)                (None, 20, 3, 256)   0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 20, 3, 256)   1024        conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_133 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 20, 3, 256)   1024        conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 20, 3, 256)   0           re_lu_131[0][0]                  \n",
      "                                                                 batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_134 (ReLU)                (None, 20, 3, 256)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 20, 3, 256)   1024        conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_136 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 20, 3, 256)   1024        conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 20, 3, 256)   0           re_lu_134[0][0]                  \n",
      "                                                                 batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_137 (ReLU)                (None, 20, 3, 256)   0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 20, 3, 512)   1573376     re_lu_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 20, 3, 512)   2048        conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_139 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 20, 3, 512)   131584      re_lu_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 20, 3, 512)   2048        conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 20, 3, 512)   2048        conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 20, 3, 512)   0           batch_normalization_151[0][0]    \n",
      "                                                                 batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_140 (ReLU)                (None, 20, 3, 512)   0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 20, 3, 512)   2048        conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_142 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 20, 3, 512)   2048        conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 20, 3, 512)   0           re_lu_140[0][0]                  \n",
      "                                                                 batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_143 (ReLU)                (None, 20, 3, 512)   0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 20, 3, 512)   2048        conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_145 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 20, 3, 512)   2048        conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 20, 3, 512)   0           re_lu_143[0][0]                  \n",
      "                                                                 batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_146 (ReLU)                (None, 20, 3, 512)   0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 10, 1, 512)   0           re_lu_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 5120)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            10242       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 29s - loss: 1.0012 - accuracy: 0.7608 - val_loss: 8.1196 - val_accuracy: 0.4706\n",
      "Epoch 2/100\n",
      "24/24 - 29s - loss: 0.6035 - accuracy: 0.8706 - val_loss: 5.2782 - val_accuracy: 0.6471\n",
      "Epoch 3/100\n",
      "24/24 - 42s - loss: 0.4977 - accuracy: 0.8889 - val_loss: 2.8033 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "24/24 - 46s - loss: 0.3094 - accuracy: 0.9059 - val_loss: 0.6465 - val_accuracy: 0.8471\n",
      "Epoch 5/100\n",
      "24/24 - 46s - loss: 0.5099 - accuracy: 0.8588 - val_loss: 1.6364 - val_accuracy: 0.8941\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "class_weight = {0: 0.83,\n",
    "                1: 0.17}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t94.44444%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       246\n",
      "           1       0.94      0.89      0.91       114\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.94      0.93      0.93       360\n",
      "weighted avg       0.94      0.94      0.94       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
