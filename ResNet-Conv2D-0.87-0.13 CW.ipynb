{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 20, 3, 60)    120         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 20, 3, 60)    240         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_147 (ReLU)                (None, 20, 3, 60)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 20, 3, 64)    46144       re_lu_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 20, 3, 64)    256         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_149 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 20, 3, 64)    3904        re_lu_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 20, 3, 64)    256         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 20, 3, 64)    256         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 20, 3, 64)    0           batch_normalization_162[0][0]    \n",
      "                                                                 batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_150 (ReLU)                (None, 20, 3, 64)    0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 20, 3, 64)    256         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_152 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 20, 3, 64)    256         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 20, 3, 64)    0           re_lu_150[0][0]                  \n",
      "                                                                 batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_153 (ReLU)                (None, 20, 3, 64)    0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 20, 3, 64)    256         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_155 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 20, 3, 64)    256         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 20, 3, 64)    0           re_lu_153[0][0]                  \n",
      "                                                                 batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_156 (ReLU)                (None, 20, 3, 64)    0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 20, 3, 128)   98432       re_lu_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 20, 3, 128)   512         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_158 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 20, 3, 128)   8320        re_lu_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 20, 3, 128)   512         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 20, 3, 128)   512         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 20, 3, 128)   0           batch_normalization_172[0][0]    \n",
      "                                                                 batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_159 (ReLU)                (None, 20, 3, 128)   0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 20, 3, 128)   512         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_161 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 20, 3, 128)   512         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 20, 3, 128)   0           re_lu_159[0][0]                  \n",
      "                                                                 batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_162 (ReLU)                (None, 20, 3, 128)   0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 20, 3, 128)   512         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_164 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_164[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 20, 3, 128)   512         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 20, 3, 128)   0           re_lu_162[0][0]                  \n",
      "                                                                 batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_165 (ReLU)                (None, 20, 3, 128)   0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_165[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 20, 3, 128)   512         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_167 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 20, 3, 128)   512         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 20, 3, 128)   0           re_lu_165[0][0]                  \n",
      "                                                                 batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_168 (ReLU)                (None, 20, 3, 128)   0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 20, 3, 256)   393472      re_lu_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 20, 3, 256)   1024        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_170 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 20, 3, 256)   33024       re_lu_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 20, 3, 256)   1024        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 20, 3, 256)   1024        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 20, 3, 256)   0           batch_normalization_185[0][0]    \n",
      "                                                                 batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_171 (ReLU)                (None, 20, 3, 256)   0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 20, 3, 256)   1024        conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_173 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 20, 3, 256)   1024        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 20, 3, 256)   0           re_lu_171[0][0]                  \n",
      "                                                                 batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_174 (ReLU)                (None, 20, 3, 256)   0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 20, 3, 256)   1024        conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_176 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 20, 3, 256)   1024        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 20, 3, 256)   0           re_lu_174[0][0]                  \n",
      "                                                                 batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_177 (ReLU)                (None, 20, 3, 256)   0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 20, 3, 256)   1024        conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_179 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 20, 3, 256)   1024        conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 20, 3, 256)   0           re_lu_177[0][0]                  \n",
      "                                                                 batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_180 (ReLU)                (None, 20, 3, 256)   0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 20, 3, 256)   1024        conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_182 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 20, 3, 256)   1024        conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 20, 3, 256)   0           re_lu_180[0][0]                  \n",
      "                                                                 batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_183 (ReLU)                (None, 20, 3, 256)   0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 20, 3, 256)   1024        conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_185 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 20, 3, 256)   1024        conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 20, 3, 256)   0           re_lu_183[0][0]                  \n",
      "                                                                 batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_186 (ReLU)                (None, 20, 3, 256)   0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 20, 3, 512)   1573376     re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 20, 3, 512)   2048        conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_188 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 20, 3, 512)   131584      re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 20, 3, 512)   2048        conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 20, 3, 512)   2048        conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 20, 3, 512)   0           batch_normalization_204[0][0]    \n",
      "                                                                 batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_189 (ReLU)                (None, 20, 3, 512)   0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 20, 3, 512)   2048        conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_191 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 20, 3, 512)   2048        conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 20, 3, 512)   0           re_lu_189[0][0]                  \n",
      "                                                                 batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_192 (ReLU)                (None, 20, 3, 512)   0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 20, 3, 512)   2048        conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_194 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 20, 3, 512)   2048        conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 20, 3, 512)   0           re_lu_192[0][0]                  \n",
      "                                                                 batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_195 (ReLU)                (None, 20, 3, 512)   0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 10, 1, 512)   0           re_lu_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 5120)         0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            10242       flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 25s - loss: 0.9767 - accuracy: 0.7791 - val_loss: 8.1196 - val_accuracy: 0.4706\n",
      "Epoch 2/100\n",
      "24/24 - 26s - loss: 0.4832 - accuracy: 0.8693 - val_loss: 1.7280 - val_accuracy: 0.8824\n",
      "Epoch 3/100\n",
      "24/24 - 43s - loss: 0.3349 - accuracy: 0.9033 - val_loss: 1.5142 - val_accuracy: 0.8824\n",
      "Epoch 4/100\n",
      "24/24 - 48s - loss: 0.3363 - accuracy: 0.9007 - val_loss: 1.1336 - val_accuracy: 0.9059\n",
      "Epoch 5/100\n",
      "24/24 - 50s - loss: 0.2839 - accuracy: 0.9059 - val_loss: 0.4810 - val_accuracy: 0.9176\n",
      "Epoch 6/100\n",
      "24/24 - 38s - loss: 0.2101 - accuracy: 0.9163 - val_loss: 0.4562 - val_accuracy: 0.9176\n",
      "Epoch 7/100\n",
      "24/24 - 28s - loss: 0.1756 - accuracy: 0.9320 - val_loss: 0.3323 - val_accuracy: 0.9529\n",
      "Epoch 8/100\n",
      "24/24 - 28s - loss: 0.1472 - accuracy: 0.9451 - val_loss: 0.3321 - val_accuracy: 0.9412\n",
      "Epoch 9/100\n",
      "24/24 - 29s - loss: 0.1197 - accuracy: 0.9608 - val_loss: 0.3193 - val_accuracy: 0.9412\n",
      "Epoch 10/100\n",
      "24/24 - 31s - loss: 0.0991 - accuracy: 0.9621 - val_loss: 1.1177 - val_accuracy: 0.8941\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "class_weight = {0: 0.87,\n",
    "                1: 0.13}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3SU9b3v8fd3LknIhYshQJgowQsioAYzWKzVWrVubK2Xarto615nde9Tdpe96e5e1Z691m57Vs8+nn12r7sXD1p7TpcePR68nNa61Val1tbSgqCAoIAiJOGScAkQCElmfuePZyaZhEAmYSbPMzOf11qz5plnnnnmywCf+c3veZ7fz5xziIhIcIX8LkBERE5NQS0iEnAKahGRgFNQi4gEnIJaRCTgIvnY6dSpU11jY2M+di0iUpTWrFnT4ZyrG+65vAR1Y2Mjq1evzseuRUSKkpm9d7Ln1PUhIhJwCmoRkYBTUIuIBFxe+qhFREart7eXlpYWuru7/S4lryoqKmhoaCAajWb9GgW1iARCS0sLNTU1NDY2YmZ+l5MXzjn27dtHS0sLs2fPzvp16voQkUDo7u6mtra2aEMawMyora0d9a8GBbWIBEYxh3TaWP6MwQnqvh545Xuw7UW/KxERCZSsgtrM7jKzjWa2wcweMbOKnFcSjsIf/w02PJ7zXYuIjOTgwYP85Cc/GfXrPvKRj3Dw4ME8VDRgxKA2sxjwZSDunFsAhIGlOa/EDGLN0LIm57sWERnJyYI6kUic8nXPPPMMkydPzldZQPZdHxFggplFgEqgLS/VxOLQvhm6D+Vl9yIiJ3PPPfewbds2mpqaWLRoER/60If49Kc/zYUXXgjAzTffTHNzM/Pnz2f58uX9r2tsbKSjo4Pt27dzwQUX8LnPfY758+dz3XXXcezYsZzUNuLpec65VjP7V2AHcAx43jn3/NDtzGwZsAzgrLPOGls1Dc2Ag7a1cPYHx7YPESl43/rVRt5sy22Dbd7MiXzjY/NP+vy9997Lhg0bWLduHStXruSjH/0oGzZs6D+N7sEHH+SMM87g2LFjLFq0iFtvvZXa2tpB+9iyZQuPPPII999/P5/85Cd5/PHHuf3220+79my6PqYANwGzgZlAlZmd8M7OueXOubhzLl5XN+wAUCOLNXv3rRrQSUT8demllw461/mHP/whF198MYsXL2bnzp1s2bLlhNfMnj2bpqYmAJqbm9m+fXtOasnmgpdrgXedc+0AZvYE8H7goZxUkGnCFKg9V/3UIiXuVC3f8VJVVdW/vHLlSn7729/y6quvUllZyVVXXTXsudDl5eX9y+FwOGddH9n0Ue8AFptZpXknAF4DbMrJuw8nFvda1JodXUTGUU1NDYcPHx72uc7OTqZMmUJlZSWbN2/mT3/607jWlk0f9SozWwG8BvQBa4Hlp37VaYg1wxuPQmcLTD4zb28jIpKptraWyy+/nAULFjBhwgSmT5/e/9ySJUu47777uOiiizj//PNZvHjxuNZmLg8t13g87sY8cUDrGrj/avjE/4T5t+S0LhEJrk2bNnHBBRf4Xca4GO7PamZrnHPx4bYPzpWJadMvhHA5tOiAoogIBDGoI2VQf5HXshYRkQAGNXgHFNvWQaLX70pERHwXzKBuiEPfMdj7pt+ViIj4LphBnb7wRf3UIiIBDeopjVBZq35qERGCGtRmXj+1WtQiElDV1dXj9l7BDGrw+qk73obuTr8rERHxVXAnt41ljqR3lc/FiEixu/vuu5k1axZ33HEHAN/85jcxM15++WUOHDhAb28v3/72t7npppvGvbaABzVe98fZV/lZiYiMt3+/B3avz+0+Z1wI19970qeXLl3KnXfe2R/Ujz32GM8++yx33XUXEydOpKOjg8WLF3PjjTeO+9yOwQ3qCZOh9jwdUBSRcbFw4UL27t1LW1sb7e3tTJkyhfr6eu666y5efvllQqEQra2t7NmzhxkzZoxrbcENavD6qbe+4I2kVwKzE4tIyilavvl02223sWLFCnbv3s3SpUt5+OGHaW9vZ82aNUSjURobG4cd3jTfgnswEbzuj6690LnT70pEpAQsXbqURx99lBUrVnDbbbfR2dnJtGnTiEajvPTSS7z33nu+1BX8FjV4/dSTxzi9l4hIlubPn8/hw4eJxWLU19fzmc98ho997GPE43GampqYO3euL3UFO6inL4BIhddPveDjflcjIiVg/fqBg5hTp07l1VdfHXa7I0eOjFdJAe/6CEeh/mJd+CIiJS2byW3PN7N1GbdDZnbneBQHeFco7tJIeiJSukYMaufcW865JudcE9AMHAWezHtlaQ3N0NcNezaO21uKiD/yMeNU0Izlzzjaro9rgG3OufE79BlLHVBsVfeHSDGrqKhg3759RR3Wzjn27dtHRUXFqF432oOJS4FHhnvCzJYBywDOOiuHZ2hMPguq6qBlDSz6j7nbr4gESkNDAy0tLbS3t/tdSl5VVFTQ0NAwqtdkHdRmVgbcCHx9uOedc8tJzU4ej8dz95WYHklPLWqRohaNRpk9e7bfZQTSaLo+rgdec87tyVcxJ9XQ7I2kd+zguL+1iIjfRhPUn+Ik3R55lx6gqe01X95eRMRPWQW1mVUCHwaeyG85JzHzEu++RQM0iUjpyaqP2jl3FKjNcy0nN2EyTJ2jfmoRKUnBvjIxU3pqriI+dUdEZDiFE9QNzXC0Aw76M3qViIhfCieoYxkj6YmIlJDCCerp8wdG0hMRKSGFE9ThKNQ3qUUtIiWncIIavIkEdr0OfT1+VyIiMm4KK6hjzZA4Dns2+F2JiMi4KaygTk/NpX5qESkhhRXUk86EqmnqpxaRklJYQW3mtap1haKIlJDCCmrw+qn3bYVjB/yuRERkXBReUPf3U2skPREpDYUX1DMvAUwHFEWkZBReUFdMhLrzdUBRREpG4QU1DEzNpZH0RKQEZDtxwGQzW2Fmm81sk5ldlu/CTqmhGY7ugwPbfS1DRGQ8ZNui/gHwrHNuLnAxsCl/JWUhpgtfRKR0jBjUZjYRuBL4GYBzrsc55+8ss9PmQbRS/dQiUhKyaVGfDbQDPzeztWb2gJlVDd3IzJaZ2WozW93e3p7zQgcJR7yR9HThi4iUgGyCOgJcAvzUObcQ6ALuGbqRc265cy7unIvX1dXluMxhNDTDrjc0kp6IFL1sgroFaHHOrUo9XoEX3P6KxVMj6a33uxIRkbwaMaidc7uBnWZ2fmrVNcCbea0qG+krFFt0QFFEilu2Z318CXjYzN4AmoB/zl9JWZoYg+rp6qcWkaIXyWYj59w6IJ7nWkbHzOv+0JkfIlLkCvPKxLSGZti/DY7u97sSEZG8KeygjmkkPREpfoUd1DMX4o2kp+4PESlehR3UFROhbq76qUWkqBV2UIPXT926RiPpiUjRKvygjsXh2H7Y/47flYiI5EXhB3WDRtITkeJW+EFdd4FG0hORolb4QR2OeGd/6MwPESlShR/UALFm2L0e+o77XYmISM4VR1A3xCHR44W1iEiRKY6gTl+hqH5qESlCxRHUk2JQU69+ahEpSsUR1OD1U6tFLSJFqHiCuiEOB96Frn1+VyIiklPFE9Tpfuo2jaQnIsUlq6A2s+1mtt7M1plZMPsXZi4EC6n7Q0SKTlYzvKR8yDnXkbdKTld5tXeVog4oikiRKZ6uD9BIeiJSlLINagc8b2ZrzGzZcBuY2TIzW21mq9vb23NX4WjE4nDsgEbSE5Gikm1QX+6cuwS4HviCmV05dAPn3HLnXNw5F6+rq8tpkVlr0IUvIlJ8sgpq51xb6n4v8CRwaT6LGrO6uVBWrX5qESkqIwa1mVWZWU16GbgO2JDvwsYkFPbO/lCLWkSKSDYt6unAK2b2OvBn4NfOuWfzW9ZpiF3iDc7U2+13JSIiOTHi6XnOuXeAi8ehltyIxSHZ64X1mYv8rkZE5LQV1+l5kDE1l7o/RKQ4FF9QT5wJNTPVTy0iRaP4ghpSF74oqEWkOBRnUMficGA7dAX3incRkWwVZ1D391Ov8bcOEZEcKM6grm/SSHoiUjSKM6jLq2HaPPVTi0hRKM6gBm9qrtY1kEz6XYmIyGkp3qBuiEN3J+zf5nclIiKnpXiDOqaR9ESkOBRvUNedr5H0RKQoFG9QayQ9ESkSxRvU4PVT79kAvcf8rkREZMyKO6hjcUj2wa43/K5ERGTMijuoNZKeiBSB4g7qmhkwsUH91CJS0LIOajMLm9laM3s6nwXlnEbSE5ECN5oW9VeATfkqJG9icTi4A460+12JiMiYZBXUZtYAfBR4IL/l5IFG0hORApdti/r7wNeAkw6cYWbLzGy1ma1ubw9Q67W+CSys7g8RKVgjBrWZ3QDsdc6dsknqnFvunIs75+J1dXU5K/C0lVXC9Hk6oCgiBSubFvXlwI1mth14FLjazB7Ka1W5FotD62saSU9ECtKIQe2c+7pzrsE51wgsBV50zt2e98pyqSEOxzth31a/KxERGbXiPo86Ldbs3aufWkQK0KiC2jm30jl3Q76KyZupc6CsRv3UIlKQSqNFHQpDbKFa1CJSkEojqME7oLhno0bSE5GCUzpB3ZAeSe91vysRERmV0glqTc0lIgWqdIK6ZjpMOlP91CJScEonqME7Ta9FY36ISGEpraBuiEPnDjiy1+9KRESyVlpBrX5qESlApRXU9RdrJD0RKTilFdRllTB9vlrUIlJQSiuoweunblurkfREpGCUXlDH4nD8EHS87XclIiJZKb2g7p+aS90fIlIYSi+oa8+D8knqpxaRglF6QR0KaSQ9ESko2cyZWGFmfzaz181so5l9azwKy6tYHPa8CT1H/a5ERGRE2bSojwNXO+cuBpqAJWa2OL9l5VlDHFwCdq3zuxIRkRFlM2eic84dST2Mpm4ur1Xlm65QFJECklUftZmFzWwdsBf4jXNu1TDbLDOz1Wa2ur29Pdd15lZ1HUw+S/3UIlIQsgpq51zCOdcENACXmtmCYbZZ7pyLO+fidXV1ua4z92JxjaQnIgVhtJPbHgRWAkvyUs14aojDoRY4vNvvSkRETimbsz7qzGxyankCcC2wOd+F5V26n7pVrWoRCbZsWtT1wEtm9gbwF7w+6qfzW9Y4qL8IQhEdUBSRwIuMtIFz7g1g4TjUMr6iE7yR9HRAUUQCrvSuTMwUi0PrWkgm/K5EROSkSjuoG+LQc1gj6YlIoJV2UOvCFxEpAKUd1LXneiPpqZ9aRAKstIM6FILYJbrwRUQCrbSDGrx+6r0boafL70pERIaloI7FwSWhTSPpiUgwKag1NZeIBJyCumoqTJ6lMz9EJLAU1OC1qjXmh4gElIIavH7qQ61waJfflYiInEBBDeqnFpFAU1ADzLgIQlH1U4tIICmoAaIVMGOB+qlFJJAU1GmxOLRpJD0RCR4FdVpDHHqOQHvhT14jIsUlm6m4zjSzl8xsk5ltNLOvjEdh404j6YlIQGXTou4DvuqcuwBYDHzBzObltywf1J4DFZN15oeIBM6IQe2c2+Wcey21fBjYBMTyXdi4M4NYs0bSE5HAGVUftZk14s2fuGqY55aZ2WozW93e3p6b6sZbQxzaN8HxI35XIiLSL+ugNrNq4HHgTufcoaHPO+eWO+fizrl4XV1dLmscP/0j6a31uxIRkX5ZBbWZRfFC+mHn3BP5LclHsWbvXv3UIhIg2Zz1YcDPgE3Oue/mvyQfVdXClNk680NEAiWbFvXlwF8DV5vZutTtI3muyz+xZl2hKCKBEhlpA+fcK4CNQy3B0BCHDSvgUBtMnOl3NSIiIwd1yWlY5N0/+FcwbT5MPQ+mzkndzoPKM/ytT0RKjoJ6qJmXwLXf9M786NgC216ExPGB5ytrB0I7M8Anz4JQ2K+qRaSIKaiHCoXgA3cNPE4m4OAOL7Q73k7dtsDmZ+DoLwa2C5dB7bleaNeelxHm50F5zfj/OUSkaCioRxIKwxmzvduc6wY/d3S/F9r7tgwE+J6NsOlpcBmj8NXMPLEFPnWO1wdupdP9LyJjo6A+HZVnwFnv826Z+nrgwLuDW+AdW+CN/wPHM64VilbB1HNPDPAzzvHGyBYRQUGdH5EyqDvfu2VyDo7sHRLgb8POVbD+/2ZsaDBlFtTNhcYr4NxrvGW1vkVKkoJ6PJlBzXTvNvuKwc/1HIX92wYH+K434O1n4fl/hIkxOOdDcM41cPZVOvtEpIQoqIOirBJmXOjdMh3c6Z15su0F2PQrWPsQWMg7O+Xca7zgjjVDWH+VIsXKnHM532k8HnerV+sy7JxL9EHba7D1BS+4W9d4g0hVTILZHxwI7sln+l2piIySma1xzsWHfU5BXcCO7od3fwdbfwtbX4TDbd76qXO8wD73Gph1uddaF5FAU1CXAue8+R7Tre33/gh93RAuh1mXDQT3tHk6KCkSQArqUtR7DN77g9fS3vbCwKS9NfVwztUDNx2UFAmEUwW1jkAVq+gEOPda7wbQ2eIdlNz6Amz+Nax7GDCYuXCgb7thkQ5KigSQWtSlKJmA1te8lvbWF7yJElwSyifB2VcOdJNMPsvvSkVKhro+5NSOHYB3fpcK7hfhUIu3vvY8L7DnLPHO3VbftsipdR+CioljeqmCWrLnnHexTfqg5PY/QN8xmH4hXHEXzLtZowSKDHXgPe/CtI6t8PlXxtSFeKqgzmYqrgfNbK+ZbRj1O0vhMfMufb/sDrj9cbh7O9x8nzfU64q/gR/FYc3/gr7jI+5KpOj1dMGL34YfLfIaNxfe6nUj5tiILWozuxI4AvzCObcgm52qRV2EkknY/DT8/juwa503IuD7vwTN/wHKqvyuTmR8OQcbHoff/BMcaoULPwHXfgsmxca8y9NqUTvnXgb2j/ndR+H+l9/hj1s7SCRz3x0jpykUgnk3wrKVcPsTUHsOPPd1+N4C+N2/eP3cIqVg1+vw8+vh8b+FqqnwN8/BrQ+cVkiPJGfnYpnZMmAZwFlnjf5sga7jffzbi1s41N3HjIkV3NQ0k1suiTF3xtg65iVPzLwDjOdeAztWwSvfhZf+C/zhB7Dob2HxF7xBp0SKTVcHvPCf4bVfeDM9feyHsPD2cTlmk9XBRDNrBJ7Od9dHd2+C327aw1NrW1n5Vjt9ScfcGTXcsjDGjU0zqZ80YdT7lHGwez288j3Y+CSEot4/3su/DFMa/a5M5PQleuHP98PKe6G3Cy79O/jg12DC5Jy+zWmf9TFeQZ1pf1cPT7/RxpNrW1m74yBmcNnZtdy8MMb1C2ZQUxE9rf1LHuzb5rWs1/1v74DKhZ/wpjWbNtfvykTGZusL8OzXoeMt7/qCJf/1xHHmc6QggzrT9o4unlrXylNrW9m+7yjlkRDXzpvOLU0xrpxTR1lkxK52GU+drfDqj2HNz6H3KMy9Aa74e284VpFCsP8deO4f4a1nYMpsL6DnLMnrtQSnFdRm9ghwFTAV2AN8wzn3s1O9Jl9nfTjnWLfzIE+tbeVXb+xif1cPUyqj3HDRTG5eGOOSsyZjuigjOLr2wZ//B6y6D7o7vYtmrviqN2uN/p4kiI4f9s5sevXH3oTVV/4DLL4DIuV5f+uivOClN5Hk91vaeXJtG89v3M3xviSzaiu5qSnGLQtjzJ6qU8YCo/uQ17r+44+gay/E4l5gz1ninU0i4rdkEtY/Br/5BhzZDRd/Cq79JtTMGLcSijKoMx3u7uW5jd5ByD9s68A5aDpzMrcsjHHDRfXUVuf/21Cy0NvtDQb1h+/DwR3ekKsfuAvmf1yDQYl/WtbAs3dDy1+87rnr/wUahs3LvCr6oM60u7ObX77eypNr29i06xDhkPHBOXXcvDDGhy+YzoQyXf7su0Sfd7HAK9/1hl+d0giXfwUu/rRmX5fxc3gPvPAtr/FQPd1rQV+01LdfeSUV1Jk27z7EU2vb+H/rWtnV2U1VWZglC+q5ZWGMy86pJRxSP6mvkknvYM3vv+NNMVY9A97/RWj+LJRX+12dFKu+Hlj1U/jdf/cm17jsDrjiH8Y8mFKulGxQpyWTjlXv7uepta08s34Xh4/3Ma2mnJuavIOQ8+on6iCkn5zzphT7/Xfg3ZehYjK87/Pwvr/TxAaSW28/551ut3+bd4zkr/7Zu8o2AEo+qDN19yZ4cfNenlzbysq39tKbcMyZXs3NC2Pc3BRj5mRdVOOrnX/xukTeegaiVRD/LFz2RZhY73dlUsg6tngBvfU33vC9S+6F8671u6pBFNQncaCrh1+v38WTa1tZ8543VsXis8/gpqYYF9RPpH5SBVOry9VF4oc9b3pXO25YAaEINH0aFtwKkQrvcSgC4ah3JWQonFqOeI/DkYzlqE4FLGXdnd5YNKvug2glfPBuuHQZRMr8ruwECuos7Nh3tP+imnc6uvrXh0PGtJpyZkyqoH5SBdMnevczJk1gRmp52sRyyiM6SJkX+9+FP/4Q1j4EiZ6x7cNCqUCPpEI8mhH0kdEvl0+E6mnerSp9X+fdayTBYEgmYd1D3tgcXR3esAbX/JP3dxRQCupRcM6xZe8Rdu4/yq7ObnZ3drOrs5s9h7rZ1XmM3Z3ddPUkTnhdbVXZScN8xiTvVl2uU9DG7PAeaN/knTGS7INkr3efyFxO3fcv93rTjg273Dd4X4nU88MuD9lnd+fJRwuMVkF13ZAAnz7MumlQVq3Wfj7sWAX//jVvON4z3wfX/zdvbtCA0+S2o2BmzJlew5zpNSfd5nB3b3+A7z40OMxbD3az5r0DHDjae8Lrasoj/aE9IzPMJ5UzY+IE6idVMLkyqgObw6mZHqxR+fp64GgHHNkLXe1wZE/G8l7vwp5922DHq3B0PzBMgygyYXCAD9dCr5rmbVM+UaE+kkNt3gUr6x+Dmnr4+P3eeDNF8LkpqMegpiJKTUWU804R5t29iVQrvLv/fne6hX6om7f3tNN++DhDh94uj4T6g3xqTTnhjH9kQ/+rD/dr6IQ1bujDwSuG+0E1dF0oBBWRMBVlYSZEvVtFNERFNMyEsvTjjPsy7/n+bVPbRMNFdBVipAwmzvRuI0n0ZYT6XjiSCvbMUD/wnnfBRVcHw4Z6uHxwC71qao4ua85BiIXCXvdS5m3QuvSyZawPD9nWBq/Pep8h7x9o6xr4/fe8Xz5XfBU+8PdFdYqngjpPKqJhZtVWMav25H2WfYkk7UeODwrx3elw7+xmU9uhE/7LnvDfapj/Z0NXDW2hn/j8cPsYWJlwju7eBN29CY71JDjWmzjhCyYb4ZBlhHmIikg61AfCf0J0YF16fTrsK6NhqisiVJenbhURalL3E6Lh4P4SCUe8S5GzuRw5mYCj+waHetfeVIs9tdzZAm1rvW6Y05GTbk/njZTonFe7S4JL3yfzMi3VSc29Aa77Npwxe/zec5woqH0UCYeonzSh4MbZds7Rm3AcS4V3d68X3ukQP96bHPR48DbJ1Dapx6ntDh7rpbtzYF36Nb2J7MIkZFBVPhDc1eUR73F/sEepLk8HfbQ/5KtSoV+T8RpfR2MMhQe6QYqBc6lbRngPCnQ3zLrM7bJ8bcUkmD7P7z9t3iioZdTMjLKIURYJMWlCfscF700kU6GdpOt4H0dSt/Ty4e7Uuu6B59LLh7v72NXZPei5bJRFQicGfsbjilQ3TlkkRFnY+xzSj6PhEGWZy5EQ0bBRnno88LqBbaKpfZSFQ8H9VTBWZqmfbEXU7eUDBbUEWjrcaiqgrub0+mSTScfR3kQquHs53N1H1/FE/3L6C+BwZvCn7ncf6uZIu/c43dLvSeT+Z300bCcJfcsI9hDlkRCRkPU/joSNSChEWcS7j4SNskHrve0jqS+GaPjE10dT26a/PE65bShENJLe3orvCyZgFNRSMkIh6+/fhtMf/CndBdSTSNLbl6Q3keR46t5b5z3Xk16X8Zy37OjpGwj94bdzw772yPE++hLec72JJH1JR19qP32J5MBy0o3LZNHhkGWEeSrkU18MwwV7+suk/0sgMrD90OcHvkBS+0u9z9DnIyEjnHGLhELefTi1zqz/sbdtyFvX/9iG3UfITjzOM96yCmozWwL8AAgDDzjn7s1rVSIFILMLiACPpJtMOvqSXqj3JRy9yeTAcirMe/rSYZ9Mhb2jL+l9mWRu23vC+oF13jbpfabWJ5L0pvabua++ZJJjvQOv63/PjO0Hah2fL5tTiYSMUGhooA/+coiEjKnV5Tz2+cty//4jbWBmYeDHwIeBFuAvZvZL59ybOa9GRHIuFDLKQlbQU9alf71kfgEM/YJJOO9XRSL1xZQc9DhJIvXrIv38wH2SRBISyeSQ9YO36Us6EglHwmWsT2S8V9JRXZ6fK5SzaVFfCmx1zr0DYGaPAjcBCmoRGRf9v15K9KBkNn/qGLAz43FLat0gZrbMzFab2er29vZc1SciUvKyCerhetFPvADOueXOubhzLl5XV3f6lYmICJBdULcAZ2Y8bgDa8lOOiIgMlU1Q/wU4z8xmm1kZsBT4ZX7LEhGRtBEPJjrn+szsi8BzeKfnPeic25j3ykREBMjyPGrn3DPAM3muRUREhlGa57qIiBQQBbWISMDlZSouM2sH3hvjy6cCHTksp5DpsxhMn8dg+jwGFMNnMcs5N+y5zXkJ6tNhZqtPNm9YqdFnMZg+j8H0eQwo9s9CXR8iIgGnoBYRCbggBvVyvwsIEH0Wg+nzGEyfx4Ci/iwC10ctIiKDBbFFLSIiGRTUIiIBF5igNrMlZvaWmW01s3v8rsdPZnammb1kZpvMbKOZfcXvmvxmZmEzW2tmT/tdi9/MbLKZrTCzzal/I7mf+6mAmNldqf8nG8zsETM7/QkxAyYQQZ0x3df1wDzgU2Y2z9+qfNUHfNU5dwGwGPhCiX8eAF8BNvldRED8AHjWOTcXuJgS/lzMLAZ8GYg75xbgDRy31N+qci8QQU3GdF/OuR4gPd1XSXLO7XLOvZZaPoz3H/GEWXVKhZk1AB8FHvC7Fr+Z2UTgSuBnAM65HufcQX+r8l0EmGBmEaCSIhwvPyhBndV0X6XIzBqBhcAqfyvx1feBrwFJvwsJgLOBduDnqa6gB8ysyu+i/OKcawX+FdgB7AI6nXPP+1tV7gUlqLOa7qvUmFk18Dhwp8acta4AAAEkSURBVHPukN/1+MHMbgD2OufW+F1LQESAS4CfOucWAl1AyR7TMbMpeL++ZwMzgSozu93fqnIvKEGt6b6GMLMoXkg/7Jx7wu96fHQ5cKOZbcfrErvazB7ytyRftQAtzrn0L6wVeMFdqq4F3nXOtTvneoEngPf7XFPOBSWoNd1XBjMzvD7ITc657/pdj5+cc193zjU45xrx/l286JwruhZTtpxzu4GdZnZ+atU1wJs+luS3HcBiM6tM/b+5hiI8uJrVDC/5pum+TnA58NfAejNbl1r3n1Iz7Yh8CXg41ah5B/isz/X4xjm3ysxWAK/hnS21liK8nFyXkIuIBFxQuj5EROQkFNQiIgGnoBYRCTgFtYhIwCmoRUQCTkEtIhJwCmoRkYD7/3t5xk9IJZTUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t90.55556%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       246\n",
      "           1       0.99      0.71      0.83       114\n",
      "\n",
      "    accuracy                           0.91       360\n",
      "   macro avg       0.93      0.85      0.88       360\n",
      "weighted avg       0.92      0.91      0.90       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
