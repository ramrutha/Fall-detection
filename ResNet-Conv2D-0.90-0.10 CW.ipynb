{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path on my system to training data\n",
    "train_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_04835861-01-2013-10-16-13-19-35.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_17744725-01-2009-02-24-16-26-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_20983985-01-2013-05-16-16-16-00.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_23112025-01-2013-05-21-06-21-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_36551836-01-2013-05-21-17-39-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_38243026-05-2009-04-29-08-53-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_42990421-03a-2011-03-23-16-50-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_47451392-01-2014-01-22-22-42-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_63414187-01-2013-05-30-22-48-27.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_67458491-01-2013-11-07-12-36-22.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_74827807-04-2009-02-09-22-39-54.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_79761947-03-2012-06-24-02-09-34.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-01-2009-02-26-12-42-02.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_91943076-02-2009-02-26-22-15-13.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-01-2011-05-18-18-57-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Train/F_96201346-05-2011-06-03-22-36-07.csv\n"
     ]
    }
   ],
   "source": [
    "#Read all training data files\n",
    "from os import walk\n",
    "\n",
    "data_for_fall=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(train_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Rel_Time</th>\n",
       "      <th>Acc_X</th>\n",
       "      <th>Acc_Y</th>\n",
       "      <th>Acc_Z</th>\n",
       "      <th>Mg_X</th>\n",
       "      <th>Mg_Y</th>\n",
       "      <th>Mg_Z</th>\n",
       "      <th>Ang_X</th>\n",
       "      <th>Ang_Y</th>\n",
       "      <th>Ang_Z</th>\n",
       "      <th>Fall_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184020.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>2.0243</td>\n",
       "      <td>-8.4086</td>\n",
       "      <td>-2.95860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>185220.0</td>\n",
       "      <td>735520.0</td>\n",
       "      <td>8.7200</td>\n",
       "      <td>-1.8686</td>\n",
       "      <td>0.31143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time  Rel_Time   Acc_X   Acc_Y    Acc_Z  Mg_X  Mg_Y  Mg_Z  Ang_X  \\\n",
       "0      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "1      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "2      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "3      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "4      184020.0  735520.0  2.0243 -8.4086 -2.95860     0     0     0      0   \n",
       "...         ...       ...     ...     ...      ...   ...   ...   ...    ...   \n",
       "23995  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23996  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23997  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23998  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "23999  185220.0  735520.0  8.7200 -1.8686  0.31143     0     0     0      0   \n",
       "\n",
       "       Ang_Y  Ang_Z  Fall_indicator  \n",
       "0          0      0               0  \n",
       "1          0      0               0  \n",
       "2          0      0               0  \n",
       "3          0      0               0  \n",
       "4          0      0               0  \n",
       "...      ...    ...             ...  \n",
       "23995      0      0               0  \n",
       "23996      0      0               0  \n",
       "23997      0      0               0  \n",
       "23998      0      0               0  \n",
       "23999      0      0               0  \n",
       "\n",
       "[24000 rows x 12 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print one file data in training\n",
    "data_for_fall[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#Creates windows of size=20 and steps forward by 1\n",
    "def create_dataset(X, Y, feature_size=20, step_size=1):\n",
    "    window_X, window_Y = [], []\n",
    "    for i in range(0, len(X) - feature_size, step_size):\n",
    "        #Feature size=20 (size of window)\n",
    "        window_data = X.iloc[i:(i + feature_size),:]\n",
    "        #Get all the 20 labels\n",
    "        labels = Y.iloc[i: i + feature_size]\n",
    "        #Append for each 20 sized windows, each stepped forward by 1\n",
    "        window_X.append(window_data)\n",
    "        #Get the max value- If window contains the row that has fall data then window labelled as fall\n",
    "        #The fall indicator can be any value >0 dependng on severity of fall\n",
    "        window_Y.append(labels.max())\n",
    "    return (window_X), np.array(window_Y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_features(data):\n",
    "    #Flattens the window data into one row\n",
    "    #Will get 20 rows of 3 columns each = 60 data points in each row\n",
    "    feature_columns = ['Acc_X','Acc_Y','Acc_Z']\n",
    "    sensor_values = data[feature_columns].values \n",
    "    features = sensor_values.flatten()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#Get the training window data without overlap from other files\n",
    "train_X_window = []\n",
    "train_Y_window = []\n",
    "#For each file in the 16 training files\n",
    "for dataset in data_for_fall:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        data_index_lists = []\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-35,fall_loc+25)))\n",
    "        #Get all the indices as a list    \n",
    "        drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X, train_each_Y = create_dataset(\n",
    "            dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "    \n",
    "    \n",
    "        #Adding only non-fall data from the files\n",
    "        data_index_lists_non_fall=[]\n",
    "        \n",
    "        #1.2s of data - 10 seconds before fall, duration is 1.2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-570)))\n",
    "        #Get all the indices as a list    \n",
    "        non_fall_indices = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "        #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "        dataset_sample_non_fall = dataset.iloc[non_fall_indices]\n",
    "\n",
    "\n",
    "        #Create windowed data for the file\n",
    "        train_each_X_non_fall, train_each_Y_non_fall = create_dataset(\n",
    "            dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "            dataset_sample_non_fall[['Fall_indicator']],\n",
    "            feature_size=20\n",
    "        )\n",
    "\n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_non_fall_flat = np.stack([to_features(d) for d in train_each_X_non_fall])  \n",
    "\n",
    "        #Flatten the windowed data to get 60 features in a row\n",
    "        train_each_X_flat = np.stack([to_features(d) for d in train_each_X])\n",
    "        #Append the data of each file\n",
    "        train_X_window.append(train_each_X_flat)\n",
    "        train_X_window.append(train_each_X_non_fall_flat)\n",
    "        train_Y_window.append(train_each_Y)\n",
    "        train_Y_window.append(train_each_Y_non_fall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_Y_window[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_window[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4014 ,  0.62286, 10.589  , -1.8686 ,  0.77857,  8.5643 ,\n",
       "        -1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ],\n",
       "       [-1.8686 ,  0.77857,  8.5643 , -1.8686 ,  0.62286,  7.7857 ,\n",
       "        -1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ],\n",
       "       [-1.8686 ,  0.62286,  7.7857 , -1.5571 ,  1.2457 ,  7.1629 ,\n",
       "        -0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ],\n",
       "       [-1.5571 ,  1.2457 ,  7.1629 , -0.77857,  1.2457 ,  7.3186 ,\n",
       "         1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ],\n",
       "       [-0.77857,  1.2457 ,  7.3186 ,  1.8686 ,  1.2457 ,  7.7857 ,\n",
       "         7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ],\n",
       "       [ 1.8686 ,  1.2457 ,  7.7857 ,  7.7857 , -1.09   ,  8.5643 ,\n",
       "         2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ],\n",
       "       [ 7.7857 , -1.09   ,  8.5643 ,  2.6471 ,  0.46714,  7.3186 ,\n",
       "        -0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ],\n",
       "       [ 2.6471 ,  0.46714,  7.3186 , -0.62286, -3.7371 , 14.481  ,\n",
       "        -1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ],\n",
       "       [-0.62286, -3.7371 , 14.481  , -1.5571 ,  7.3186 , 11.056  ,\n",
       "        -2.0243 , -6.2286 , 10.433  , -3.27   , -1.5571 ,  9.81   ,\n",
       "        -1.7129 ,  0.     ,  9.81   , -0.77857,  0.15571,  9.4986 ,\n",
       "         0.15571, -0.93429,  9.4986 , -0.77857, -1.2457 ,  9.6543 ,\n",
       "        -0.77857, -1.2457 ,  9.4986 , -0.77857, -0.77857,  9.6543 ,\n",
       "        -0.46714, -0.15571,  9.81   , -1.8686 , -2.4914 ,  9.4986 ,\n",
       "        -3.27   , -0.15571, 10.9    , -0.62286,  0.93429, 10.433  ,\n",
       "        -0.77857, -0.93429, 10.277  , -0.31143,  0.77857, 11.834  ,\n",
       "         0.62286,  1.5571 , 10.277  , -2.18   ,  2.18   , 11.056  ,\n",
       "        -2.9586 ,  0.46714, 11.523  , -1.5571 , -0.46714, 11.367  ],\n",
       "       [-1.5571 ,  7.3186 , 11.056  , -2.0243 , -6.2286 , 10.433  ,\n",
       "        -3.27   , -1.5571 ,  9.81   , -1.7129 ,  0.     ,  9.81   ,\n",
       "        -0.77857,  0.15571,  9.4986 ,  0.15571, -0.93429,  9.4986 ,\n",
       "        -0.77857, -1.2457 ,  9.6543 , -0.77857, -1.2457 ,  9.4986 ,\n",
       "        -0.77857, -0.77857,  9.6543 , -0.46714, -0.15571,  9.81   ,\n",
       "        -1.8686 , -2.4914 ,  9.4986 , -3.27   , -0.15571, 10.9    ,\n",
       "        -0.62286,  0.93429, 10.433  , -0.77857, -0.93429, 10.277  ,\n",
       "        -0.31143,  0.77857, 11.834  ,  0.62286,  1.5571 , 10.277  ,\n",
       "        -2.18   ,  2.18   , 11.056  , -2.9586 ,  0.46714, 11.523  ,\n",
       "        -1.5571 , -0.46714, 11.367  , -1.8686 ,  0.31143,  9.0314 ]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_window[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n",
      "40\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for each_window in train_Y_window:\n",
    "    print(len(each_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2457 ,  1.5571 ,  7.9414 , ..., -6.8514 ,  1.8686 ,  9.81   ],\n",
       "       [-1.09   ,  1.7129 ,  7.3186 , ..., -2.6471 , -0.93429, 11.523  ],\n",
       "       [-0.77857,  1.4014 ,  6.8514 , ...,  2.4914 , -3.1143 ,  8.5643 ],\n",
       "       ...,\n",
       "       [-1.6167 , -1.1119 , 10.218  , ..., -1.0158 ,  0.54891,  9.7729 ],\n",
       "       [-1.5595 , -1.14   , 10.246  , ..., -1.0158 ,  0.46446,  9.8841 ],\n",
       "       [-1.5309 , -1.1682 , 10.329  , ..., -0.90136,  0.43631, 10.051  ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the windowed and flattened data of each file to form the training data\n",
    "train_X = np.concatenate(train_X_window,axis = 0)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 60)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape according to LSTM\n",
    "# X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "X_train = train_X.reshape(train_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 20, 3, 1)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate the fall indicator of each file to form the training labels\n",
    "train_Y = np.concatenate(train_Y_window, axis=0)\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "train_Y[train_Y > 1] = 1\n",
    "len(np.argwhere(train_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(train_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-01-2011-02-19-15-59-57.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_42990421-02-2011-02-19-22-58-03.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-01-2008-06-26-07-27-49.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_72858619-02-2008-06-26-11-29-16.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_74827807-07-2009-02-16-19-10-44.csv\n",
      "./../Datasets/Fall examples/Fall examples/Farseeing/Data/Test/F_96201346-03-2011-05-21-06-19-42.csv\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "# columns = ['Time','Acc_X','Acc_Y','Acc_Z','Fall_indicator']\n",
    "# train_data = pd.DataFrame() #columns=columns)\n",
    "data_for_fall_test=[]\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(test_path):\n",
    "    \n",
    "    for file in filenames:\n",
    "        print(dirpath+file)\n",
    "        data = pd.read_csv(dirpath+file)\n",
    "        data.columns = ['Time','Rel_Time','Acc_X','Acc_Y','Acc_Z','Mg_X','Mg_Y','Mg_Z','Ang_X','Ang_Y','Ang_Z','Fall_indicator']\n",
    "        data_for_fall_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME TESTING SET\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "#Same process of split for testing data\n",
    "test_X_window = []\n",
    "test_Y_window = []\n",
    "for dataset in data_for_fall_test:\n",
    "    #Get the fall event location in the file (only one file at a time)\n",
    "    fall_event_loc = dataset[dataset['Fall_indicator']>0].index\n",
    "    data_index_lists = []\n",
    "    #Get 2 seconds of data around fall event\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 1.5sec before fall, 0.5sec after fall\n",
    "        data_index_lists.append(list(range(fall_loc-40,fall_loc+20)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X, test_each_Y = create_dataset(\n",
    "        dataset_sample[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Adding only non-fall data from the files\n",
    "    data_index_lists_non_fall=[]\n",
    "    for fall_loc in fall_event_loc:\n",
    "        #2s of data - 10 seconds before fall, duration is 2 seconds\n",
    "        data_index_lists_non_fall.append(list(range(fall_loc-600,fall_loc-560)))\n",
    "    #Get all the indices as a list    \n",
    "    drop_arr = list(chain.from_iterable(data_index_lists_non_fall))\n",
    "    #Take only those indices that correspond to 2 seconds of data around fall event\n",
    "    dataset_sample_non_fall = dataset.iloc[drop_arr]\n",
    "    \n",
    "\n",
    "    #Create windowed data for the file\n",
    "    test_each_X_non_fall, test_each_Y_non_fall = create_dataset(\n",
    "        dataset_sample_non_fall[['Acc_X', 'Acc_Y', 'Acc_Z']],\n",
    "        dataset_sample_non_fall[['Fall_indicator']],\n",
    "        feature_size=20\n",
    "    )\n",
    "    \n",
    "    #Flatten the windowed data to get 60 features in a row\n",
    "    test_each_X_flat = np.stack([to_features(d) for d in test_each_X])\n",
    "    test_each_X_flat_non_fall = np.stack([to_features(d) for d in test_each_X_non_fall])\n",
    "    \n",
    "    #Append the data of each file\n",
    "    test_X_window.append(test_each_X_flat)\n",
    "    test_X_window.append(test_each_X_flat_non_fall)\n",
    "    \n",
    "    \n",
    "    test_Y_window.append(test_each_Y)\n",
    "    test_Y_window.append(test_each_Y_non_fall)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.1077 , -0.60521,  7.354  , ..., -4.7643 , -0.54891,  6.4087 ],\n",
       "       [-5.1077 , -0.57706,  7.6599 , ..., -4.564  , -0.49261,  6.3253 ],\n",
       "       [-4.8502 , -0.40816,  7.2428 , ..., -4.3923 , -0.35187,  6.1307 ],\n",
       "       ...,\n",
       "       [-0.7869 , -1.6186 ,  9.7173 , ..., -0.87274, -1.5904 ,  9.7173 ],\n",
       "       [-0.7869 , -1.6467 ,  9.6617 , ..., -0.87274, -1.6186 ,  9.7173 ],\n",
       "       [-0.75828, -1.6749 ,  9.6061 , ..., -0.87274, -1.6186 ,  9.6617 ]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = np.concatenate(test_X_window, axis = 0)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 60)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "X_test = test_X.reshape(test_X.shape[0],20,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = np.concatenate(test_Y_window, axis=0)\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all labels of fall_indicator > 1 to 1\n",
    "test_Y[test_Y > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(test_Y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "enc = enc.fit(train_Y)\n",
    "y_train = enc.transform(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet identity block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,ReLU,add,MaxPool2D,AveragePooling2D,Flatten,Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_model():\n",
    "    def identity_block(X, filters, stride, kernel_size=(4,1)):\n",
    "        out = Conv2D(filters, (1,1), stride, padding='same')(X)\n",
    "        out = ReLU()(out)\n",
    "        out = Conv2D(filters, kernel_size, stride, padding='same')(X)\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "#         out = MaxPool2D(pool_size=(2, 1))(out)\n",
    "        return out\n",
    "\n",
    "    kernels = 32\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    X = Conv2D(kernels, (2,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    X = MaxPool2D(pool_size=(2, 1))(X)\n",
    "    print(X.shape)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = identity_block(X, kernels, stride)\n",
    "    X = AveragePooling2D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_trial_2_model():\n",
    "    def identity_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        #Add shortcut\n",
    "        out = add([X, out])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def conv_block(X, kernels, kernel_size=(4,3) ,stride=(1,1)):\n",
    "        #Block 1\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 2\n",
    "        out = Conv2D(kernels, kernel_size, stride, padding='same')(X)\n",
    "        out = BatchNormalization()(out)\n",
    "        out = ReLU()(out)\n",
    "        #Block 3\n",
    "        out = Conv2D(kernels, (1,1), stride, padding='same')(out)\n",
    "        out = BatchNormalization()(out)\n",
    "        \n",
    "        #Define shortcut\n",
    "        X_shortcut = Conv2D(kernels, (1,1), stride, padding='same')(X)\n",
    "        X_shortcut = BatchNormalization()(X_shortcut)\n",
    "        \n",
    "        #Add shortcut\n",
    "        out = add([out, X_shortcut])\n",
    "        out = ReLU()(out)\n",
    "        #out = MaxPool1D()(out)\n",
    "        return out\n",
    "\n",
    "#     kernels = [60,60,256]\n",
    "    stride = (1,1)\n",
    "    \n",
    "    inputs = Input([20,3,1])\n",
    "    #Layer 1\n",
    "    X = Conv2D(60, (1,1), stride)(inputs)\n",
    "    print(X.shape)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "#     X = MaxPool2D(pool_size=(1,1))(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    #Layer 2\n",
    "    X = conv_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    X = identity_block(X, kernels = 64)\n",
    "    \n",
    "    \n",
    "    #Layer 3\n",
    "    X = conv_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    X = identity_block(X, kernels = 128)\n",
    "    \n",
    "    \n",
    "    #Layer 4\n",
    "    X = conv_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "    X = identity_block(X, kernels = 256)\n",
    "   \n",
    "    \n",
    "    #Layer 5\n",
    "    X = conv_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    X = identity_block(X, kernels = 512)\n",
    "    \n",
    "    X = AveragePooling2D()(X)\n",
    "    print(X.shape)\n",
    "    X = Flatten()(X)\n",
    "    print(X.shape)\n",
    "    output = Dense(2, activation='softmax')(X)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_resnet_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 3, 60)\n",
      "(None, 20, 3, 60)\n",
      "(None, 10, 1, 512)\n",
      "(None, 5120)\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet_trial_2_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 20, 3, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 20, 3, 60)    120         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 20, 3, 60)    240         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_196 (ReLU)                (None, 20, 3, 60)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 20, 3, 64)    46144       re_lu_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 20, 3, 64)    256         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_198 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 20, 3, 64)    3904        re_lu_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 20, 3, 64)    256         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 20, 3, 64)    256         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 20, 3, 64)    0           batch_normalization_215[0][0]    \n",
      "                                                                 batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_199 (ReLU)                (None, 20, 3, 64)    0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 20, 3, 64)    256         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_201 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 20, 3, 64)    256         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 20, 3, 64)    0           re_lu_199[0][0]                  \n",
      "                                                                 batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_202 (ReLU)                (None, 20, 3, 64)    0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 20, 3, 64)    49216       re_lu_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 20, 3, 64)    256         conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_204 (ReLU)                (None, 20, 3, 64)    0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 20, 3, 64)    4160        re_lu_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 20, 3, 64)    256         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 20, 3, 64)    0           re_lu_202[0][0]                  \n",
      "                                                                 batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_205 (ReLU)                (None, 20, 3, 64)    0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 20, 3, 128)   98432       re_lu_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 20, 3, 128)   512         conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_207 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 20, 3, 128)   8320        re_lu_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 20, 3, 128)   512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 20, 3, 128)   512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 20, 3, 128)   0           batch_normalization_225[0][0]    \n",
      "                                                                 batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_208 (ReLU)                (None, 20, 3, 128)   0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 20, 3, 128)   512         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_210 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 20, 3, 128)   512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 20, 3, 128)   0           re_lu_208[0][0]                  \n",
      "                                                                 batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_211 (ReLU)                (None, 20, 3, 128)   0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 20, 3, 128)   512         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_213 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 20, 3, 128)   512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 20, 3, 128)   0           re_lu_211[0][0]                  \n",
      "                                                                 batch_normalization_232[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_214 (ReLU)                (None, 20, 3, 128)   0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 20, 3, 128)   196736      re_lu_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 20, 3, 128)   512         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_216 (ReLU)                (None, 20, 3, 128)   0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 20, 3, 128)   16512       re_lu_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 20, 3, 128)   512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 20, 3, 128)   0           re_lu_214[0][0]                  \n",
      "                                                                 batch_normalization_235[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_217 (ReLU)                (None, 20, 3, 128)   0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 20, 3, 256)   393472      re_lu_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 20, 3, 256)   1024        conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_219 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_238 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_219[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 20, 3, 256)   33024       re_lu_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 20, 3, 256)   1024        conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 20, 3, 256)   1024        conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 20, 3, 256)   0           batch_normalization_238[0][0]    \n",
      "                                                                 batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_220 (ReLU)                (None, 20, 3, 256)   0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 20, 3, 256)   1024        conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_222 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_241[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 20, 3, 256)   1024        conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 20, 3, 256)   0           re_lu_220[0][0]                  \n",
      "                                                                 batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_223 (ReLU)                (None, 20, 3, 256)   0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 20, 3, 256)   1024        conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_225 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 20, 3, 256)   1024        conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 20, 3, 256)   0           re_lu_223[0][0]                  \n",
      "                                                                 batch_normalization_245[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_226 (ReLU)                (None, 20, 3, 256)   0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 20, 3, 256)   1024        conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_228 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_228[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 20, 3, 256)   1024        conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 20, 3, 256)   0           re_lu_226[0][0]                  \n",
      "                                                                 batch_normalization_248[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_229 (ReLU)                (None, 20, 3, 256)   0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_229[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchN (None, 20, 3, 256)   1024        conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_231 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_250[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_231[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchN (None, 20, 3, 256)   1024        conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 20, 3, 256)   0           re_lu_229[0][0]                  \n",
      "                                                                 batch_normalization_251[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_232 (ReLU)                (None, 20, 3, 256)   0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 20, 3, 256)   786688      re_lu_232[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchN (None, 20, 3, 256)   1024        conv2d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_234 (ReLU)                (None, 20, 3, 256)   0           batch_normalization_253[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 20, 3, 256)   65792       re_lu_234[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchN (None, 20, 3, 256)   1024        conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 20, 3, 256)   0           re_lu_232[0][0]                  \n",
      "                                                                 batch_normalization_254[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_235 (ReLU)                (None, 20, 3, 256)   0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 20, 3, 512)   1573376     re_lu_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchN (None, 20, 3, 512)   2048        conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_237 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_256[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_237[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 20, 3, 512)   131584      re_lu_235[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchN (None, 20, 3, 512)   2048        conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchN (None, 20, 3, 512)   2048        conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 20, 3, 512)   0           batch_normalization_257[0][0]    \n",
      "                                                                 batch_normalization_258[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_238 (ReLU)                (None, 20, 3, 512)   0           add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_238[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchN (None, 20, 3, 512)   2048        conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_240 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_260[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_240[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchN (None, 20, 3, 512)   2048        conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 20, 3, 512)   0           re_lu_238[0][0]                  \n",
      "                                                                 batch_normalization_261[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_241 (ReLU)                (None, 20, 3, 512)   0           add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 20, 3, 512)   3146240     re_lu_241[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchN (None, 20, 3, 512)   2048        conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_243 (ReLU)                (None, 20, 3, 512)   0           batch_normalization_263[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 20, 3, 512)   262656      re_lu_243[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchN (None, 20, 3, 512)   2048        conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 20, 3, 512)   0           re_lu_241[0][0]                  \n",
      "                                                                 batch_normalization_264[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_244 (ReLU)                (None, 20, 3, 512)   0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 10, 1, 512)   0           re_lu_244[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5120)         0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            10242       flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,508,714\n",
      "Trainable params: 14,491,570\n",
      "Non-trainable params: 17,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 - 51s - loss: 1.3419 - accuracy: 0.7569 - val_loss: 6.1190 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "24/24 - 32s - loss: 0.6040 - accuracy: 0.8353 - val_loss: 2.5149 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "24/24 - 29s - loss: 0.5644 - accuracy: 0.8667 - val_loss: 1.0637 - val_accuracy: 0.8941\n",
      "Epoch 4/100\n",
      "24/24 - 29s - loss: 0.5352 - accuracy: 0.8510 - val_loss: 1.6712 - val_accuracy: 0.8824\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, to_categorical(Y_train), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "\n",
    "class_weight = {0: 0.9,\n",
    "                1: 0.1}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2, validation_split=0.1, callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1f3/8deZLAQStiQEMBEDuCD7EmhARanWvdYqInWrthVrtVXbb1ttf13st1Z//XZTq21xq1a+KuJa6m5BrCwaEBBFZREkIJAEEggQSDLn+8eZrGaZkJncuTPv5+ORRybcO5PP5T5438s5555jrLWIiEjsCnhdgIiItE1BLSIS4xTUIiIxTkEtIhLjFNQiIjEuORofmp2dbfPz86Px0SIicWn58uWl1tp+LW2LSlDn5+dTVFQUjY8WEYlLxpjNrW1T04eISIxTUIuIxDgFtYhIjItKG7WISEdVV1dTXFxMVVWV16VEVVpaGnl5eaSkpIT9HgW1iMSE4uJievbsSX5+PsYYr8uJCmstZWVlFBcXM3jw4LDfp6YPEYkJVVVVZGVlxW1IAxhjyMrK6vD/GhTUIhIz4jmk6xzOMcZOUAeDsOh3sG2l15WIiMSU2AnqgxVQ9BA8cRnsK/O6GhFJMOXl5dx7770dft/ZZ59NeXl5FCpqEFZQG2P6GGPmGWM+NMasNcZMjngl3fvCxf+Ayp0w70qorYn4rxARaU1rQV1bW9vm+1544QX69OkTrbKA8O+o7wRestYOA8YAa6NSTe54OPeP8MkieP2XUfkVIiItufnmm9mwYQNjx45l4sSJTJs2jUsuuYRRo0YBcP755zNhwgRGjBjB7Nmz69+Xn59PaWkpmzZt4vjjj+fqq69mxIgRnH766Rw4cCAitbU7PM8Y0wuYClwJYK09BByKyG9vybhLYdsKWHw3DBwLo6ZH7VeJSGy69Z/v88G2PRH9zOFH9OIXXx7R6vY77riDNWvWsHLlShYuXMg555zDmjVr6ofRPfjgg2RmZnLgwAEmTpzIhRdeSFZWVpPPWLduHY899hj33XcfM2bM4KmnnuKyyy7rdO3h3FEPAUqAh4wx7xpj7jfGpDffyRgzyxhTZIwpKikp6VxVZ9wORxbC89+F7Ws691kiIodh0qRJTcY633XXXYwZM4bCwkK2bNnCunXrPveewYMHM3bsWAAmTJjApk2bIlJLOA+8JAPjge9aa5cZY+4EbgZ+1ngna+1sYDZAQUFB51bMTU6FGY/A36bCE5fC1QugR2anPlJE/KOtO9+ukp7ecD+6cOFCXnvtNZYsWUKPHj045ZRTWhwL3a1bt/rXSUlJEWv6COeOuhgottYuC/08Dxfc0dWzv+tcrNgKT18NwbYb9EVEOqNnz57s3bu3xW0VFRX07duXHj168OGHH7J06dIura3doLbWbge2GGOOC/3RqcAHUa2qzpGT4Oz/gfWvwYLbuuRXikhiysrK4oQTTmDkyJH88Ic/bLLtzDPPpKamhtGjR/Ozn/2MwsLCLq3NWNt+K4UxZixwP5AKbASustbubm3/goICG9GFA57/Lqx4BGb8A4afF7nPFZGYsXbtWo4//nivy+gSLR2rMWa5tbagpf3DmpTJWrsSaPEDusTZv4MdH8Cz10L2sZAzzLNSRES6Wuw8mdiW5G6uczGlOzx+CVRVeF2RiEiX8UdQA/TOhYsehvLN8PQ1bm4QEZEE4J+gBsg/Ac74DXz8Iiz6rdfViIh0CX8FNcCkWTDma7DwdvjoJa+rERGJOv8FtTFuPpCBY9z46tL1XlckIhJV/gtqcJ2KFz8KSSmuc/Fgy4PURUSiJSMjo8t+lz+DGqDPIJj+EJStc8P2whgPLiLiR/4NaoAhJ8OXfgVr/wn/+YPX1YiIj/34xz9uMh/1L3/5S2699VZOPfVUxo8fz6hRo3juuec8qc3/q5BPvh62vQuv/zcMGAPHnOZ1RSLSWS/eDNvfi+xnDhgFZ93R6uaZM2dy44038p3vfAeAuXPn8tJLL3HTTTfRq1cvSktLKSws5LzzzuvytR39H9TGwHl3Q8lH8NQ3YdZCyAx/GXYREYBx48axc+dOtm3bRklJCX379mXgwIHcdNNNLFq0iEAgwNatW9mxYwcDBgzo0tr8H9QAqelupr3Z09yai998xf2ZiPhTG3e+0TR9+nTmzZvH9u3bmTlzJnPmzKGkpITly5eTkpJCfn5+i9ObRpu/26gbyxwCFz4AO953kzipc1FEOmjmzJk8/vjjzJs3j+nTp1NRUUFOTg4pKSksWLCAzZs3e1JX/AQ1uPbpU38Ga56CJX/2uhoR8ZkRI0awd+9ecnNzGThwIJdeeilFRUUUFBQwZ84chg3zZkK4+Gj6aOzE78O2lfDqz2HAaDcyREQkTO+919CJmZ2dzZIlS1rcr7KysqtKirM7anCdi+ff66ZDffJKKP/U64pERDol/oIaoFtPuHgOBGtc52J1ZNYtExHxQnwGNUD20XDBffDZKph/kzoXRXwgnBWn/O5wjjF+gxrguDPhlFtg1WPw9n1eVyMibUhLS6OsrCyuw9paS1lZGWlpaR16X/x1JjY39Ueuc/HlW6D/CDentYjEnLy8PIqLiykpKfG6lKhKS0sjLy+vQ+8Ja3Hbjor44radVVUB933RfZ/1hlstRkQkhrS1uG18N33USevtOherD8Dcy6HmoNcViYiELTGCGtzK5ef/BbYuhxf+y+tqRETCljhBDTD8PDjpB7DiESh6yOtqRETCklhBDTDtpzD0VHjhh7Dlba+rERFpV+IFdSAJLrzfdSg+cTns3e51RSIibUq8oAbokek6Fw/ugblfh5pDXlckItKqxAxqgAEj3YIDW5bCyz/xuhoRkVaF9cCLMWYTsBeoBWpaG+vnO6Omu2W8lvwZjhgL4y7zuiIRkc/pyJOJ06y1pVGrxCun3QrbV8P870POcMgd73VFIiJNJG7TR52kZJj+d8jo7zoXK+P78VUR8Z9wg9oCrxhjlhtjZrW0gzFmljGmyBhT5Ltn9dOz3JqL+0th3lVQW+N1RSIi9cIN6hOsteOBs4DrjDFTm+9grZ1trS2w1hb069cvokV2iSPGwpfvhE1vutVhRERiRFhBba3dFvq+E3gGmBTNojwzZiZMugaW3gOrn/S6GhERIIygNsakG2N61r0GTgfWRLswz5xxGwya4lYy/2y119WIiIR1R90f+I8xZhXwNvAva+1L0S3LQ0kpMONh6N4XnrgU9u/yuiIRSXDtBrW1dqO1dkzoa4S19rauKMxTGTmuc3Hvdpj3DQjWel2RiCQwDc9rTV4BnPN72LgAXv+V19WISAJTULdl/BUw4Sp460/w/jNeVyMiCUpB3Z6z/j/kTYJnr4MdH3hdjYgkIAV1e5K7wYxHoFuG61w8UO51RSKSYBTU4eg10IV1+afw9NUQDHpdkYgkEAV1uAYVwpl3wLpXYOHtXlcjIglEQd0RE78FYy+DRb+FtfO9rkZEEoSCuiOMcUP2jhgHz3wbSj72uiIRSQAK6o5KSYOLH3WdjI9fAlV7vK5IROKcgvpw9M6Di/4OuzbCs9eqc1FEokpBfbgGnwSn/xo+nA9v/t7rakQkjimoO6PwWhg1AxbcBh+/4nU1IhKnFNSdYYxbbGDASHjqW1C2weuKRCQOKag7K7WH61wMBODxS+FgpdcViUicUVBHQt98mP4glH4Ez10H1npdkYjEEQV1pAz9Ipz6C/jgWXjrTq+rEZE4oqCOpBNugOHnw+u3woZ/e12NiMQJBXUkGQNfuQf6DXMrw+ze5HVFIhIHFNSR1i3DdS7aIDxxGRza73VFIuJzCupoyBoKF9wP29fAP29Q56KIdIqCOlqOPR2m/RTemwvL/up1NSLiYwrqaDrpB3DcOfDyT+GTN72uRkR8SkEdTYEAfPWvkDkEnrwSKoq9rkhEfEhBHW1pvWDm/0LNQde5WF3ldUUi4jMK6q7Q71i44G+w7V341w/UuSgiHaKg7irDzoGpP4KVj0LRA15XIyI+EnZQG2OSjDHvGmO0WODhOuUWOOZ0ePHH8OlSr6sREZ/oyB31DcDaaBWSEAIBuOA+6DMI5l4Bez7zuiIR8YGwgtoYkwecA9wf3XISQPc+cPEcNx3q3CtcJ6OISBvCvaP+E/AjoNXFAY0xs4wxRcaYopKSkogUF7f6D4fz74Hit10ziIhIG9oNamPMucBOa+3ytvaz1s621hZYawv69esXsQLj1oivutn2lj8Eyx/2uhoRiWHh3FGfAJxnjNkEPA580RjzaFSrShSn/gKGTIMX/guKi7yuRkRiVLtBba29xVqbZ63NB2YC/7bWXhb1yhJBIMmtDNNzADxxOVTu9LoiEYlBGkfttR6ZrnPxwG6Y+3Worfa6IhGJMR0KamvtQmvtudEqJmENHA3n3Q2fLoZX/p/X1YhIjEn2ugAJGX0RbFsBS++FgWNh7Ne8rkhEYoSaPmLJl34F+SfB/Bth20qvqxGRGKGgjiVJKTD9IeiR5Wba21fmdUUiEgMU1LEmox9c/A83AmTelVBb43VFIuIxBXUsyp0A5/4RPlkEr//S62pExGPqTIxV4y51nYuL73adi6Ome12RiHhEd9Sx7Izb4chCeP67bkVzEUlICupYlpwKMx6Bbr3giUth/y6vKxIRDyioY13P/q5zsWIrPH01BGu9rkhEupiC2g+OnARn/xbWvwYLbvO6GhHpYgpqv5hwFYy/At78PXzwvNfViEgXUlD7hTFw9u8gtwCevRZ2fuh1RSLSRRTUfpLczXUupnSHxy+BqgqvKxKRLqCg9pveuXDRw1C+GZ6+BoKtro4mInFCQe1H+SfAGb+Bj1+ERb/1uhoRiTIFtV9NmgVjvgYLb4ePXvK6GhGJIgW1Xxnj5gMZOMaNry5d73VFIhIlCmo/S+kOFz8KgWTXuXhwr9cViUgUKKj9rs8guOjvULbODduz1uuKRCTCFNTxYMjJbnWYtf+E//zB62pEJMIU1PFi8vUw8kJ4/b9h3WteVyMiEaSgjhfGuJXM+4+Ap74Juz7xuiIRiRAFdTxJTXcz7YFbc/HQPm/rEZGIUFDHm8whcOEDsON9t+CAOhdFfE9BHY+OOQ1O/RmseQqW/NnrakSkkxTU8erE78PxX4ZXfw4b3/C6GhHpBAV1vDIGzv8LZB0DT14J5Z96XZGIHKZ2g9oYk2aMedsYs8oY874x5tauKEwioFtPmPm/EKxxnYvVB7yuSEQOQzh31AeBL1prxwBjgTONMYXRLUsiJvtouOA++GwVzL9JnYsiPtRuUFunMvRjSuhL/9r95Lgz4ZRbYNVj8PZ9XlcjIh0UVhu1MSbJGLMS2Am8aq1d1sI+s4wxRcaYopKSkkjXKZ019Udw7Fnw8i2w6S2vqxGRDggrqK21tdbasUAeMMkYM7KFfWZbawustQX9+vWLdJ3SWYEAXPA36JsPT34dKrZ6XZGIhKlDoz6steXAQuDMqFQj0ZXWGy6e4zoV514ONQe9rkhEwhDOqI9+xpg+odfdgdMALYHtVznD3LC9rcvhhf/yuhoRCUM4d9QDgQXGmNXAO7g26vnRLUuiavh5cNIPYMUjUPSQ19WISDuS29vBWrsaGNcFtUhXmvZT2LYSXvihm3HvyEleVyQirdCTiYkqkAQX3g+9c+GJy2Hvdq8rEpFWKKgTWY9M17l4cA/M/TrUHPK6IhFpgYI60Q0Y6RYc2LIUXv6J19WISAvabaOWBDBqOmx7102JesRYGHeZ1xWJSCO6oxbntFth8FSY/33YusLrakSkEQW1OEnJMP3vkJHjOhcrNQ2ASKxQUEuD9Cy4+FHYXwrzroLaGq8rEhEU1NLcEWPhy3fCpjfd6jAi4jl1JsrnjZnp2qmX3gNHjIPRF3ldkUhC0x21tOyM22DQFLeS+Werva5GJKEpqKVlSSkw42Ho3hfmXATvzlGbtYhHFNTSuowcuOQJ6NkfnvsO3DMRVj0OwVqvKxNJKApqadvA0TDrDbdIbko6PHMN3PMFWP2kAlukiyiopX3GwLBz4JpFMOMfrlnk6W/BvZNhzdMQDHpdoUhcU1BL+AIBN5f1t9+C6Q+5AJ93Ffz1BPjgOQW2SJQoqKXjAgEYeQFcuxgufABqq2HuFfC3qbB2PlgtUi8SSQpqOXyBJDeh03XL4KuzoXofPHEpzD4ZPnpRgS0SIQpq6bxAEoy5GK57x63HWFUBj82E+6bBulcV2CKdpKCWyElKhrGXwPVFcN6fYX8ZzJkO958G619XYIscJgW1RF5SCoy/HK5fDuf+yS3z9egF8OAZsHGhAlukgxTUEj3JqVBwFXxvBZzzeyjfAo98Bf5+Dmz6j9fVifiGglqiL7kbTPwWfO9dOOu3ULbBhfXfz4XNi72uTiTmKail66SkwReugRtWwhm3Q8lH8NBZ7i57y9teVycSsxTU0vVSusPk78ANq+D0X8P2NfDAl+DRC6F4udfVicQcBbV4J7UHTPku3Ljardm4dQXc/0WYM8MttisigIJaYkFqOpx4owvsU38OW5bB7FPgsa/BZ6u8rk7Ec+0GtTHmSGPMAmPMWmPM+8aYG7qiMElA3XrCST+AG9+DaT+FzW+5x9KfuAx2vO91dSKeCeeOugb4gbX2eKAQuM4YMzy6ZUlCS+sFJ/8IblgNJ98MG9+Av0yBuV+HnWu9rk6ky7Ub1Nbaz6y1K0Kv9wJrgdxoFyZC9z4w7RbXJDL1h7D+NTe16rxvuBEjIgnC2A48JWaMyQcWASOttXuabZsFzAIYNGjQhM2bN0euShGA/btg8d2w7G9QvR9GXQQn/xiyj/a6MpFOM8Yst9YWtLgt3KA2xmQAbwC3WWufbmvfgoICW1RU1OFCRcKyrxQW3wVv3wc1VTD6YnfHnTXU68pEDltbQR3WqA9jTArwFDCnvZAWibr0bPjSr9w47MLvwPvPwJ8nwnPXwe5NXlcnEnHhjPowwAPAWmvtH6JfkkiYMnLgjNtcYE+a5dZxvHsCPP89KP/U6+pEIiacO+oTgMuBLxpjVoa+zo5yXSLh6zkAzrrDPZpe8A1Y9RjcNR7m3wQVxV5XJ9JpHepMDJfaqMVTFcXw5u9hxT/cuo4TroQTvw+9BnpdmUirOt1GLeIrvfPg3D+66VXHfA2KHoQ7x8CLN8PeHV5XJ9JhCmqJX30GwXl3wXeXw+iL4O3ZLrBf/ilU7vS6OpGwKagl/vXNh6/cA9e/AyO+CkvvdYH96s9hX5nX1Ym0S0EtiSNrKHz1L24R3mHnwlt3wZ9GwWu3uodpRGKUgloST/bRcOF9cN0yOO5M+M8f4U+j4d+/hgO7va5O/MpaqCyJykdr1IfIzrWw8A744Fno1ss9RFN4rZtrRKQ1+0ph63L3VVwE21ZAagbctOawPq6tUR/JnSpUJB7kHA8zHnYrzbxxh/ta9heYfD184dtuNj9JbIf2u7nR64J5a1HDQ1UmADnD4fgvQ24BBIMQiGxjhe6oRZr7bJW7w/7oBeje161CM2mWmy9b4l+w1s3OuLWoIZh3fAC21m3vPQhyx0PuBMgrgIFj3OIXnRSRSZk6QkEtcWHrChfY616G7plwwg0w6eqI/KOUGGEt7Nna0HyxdYVbBq56n9ue1tsFcuOvjJyolKKgFumM4uWw8DduPuwe2W7ZsIJvujUfxV8OlLsgrm/CWA6VoYegklJhwCjXfFEXyplDIt6M0RoFtUgkbHkbFvwGNi6A9Bw48SYouMqtqi6xp+YQ7HjP3SXX3TGXrWvYnnVMQ/NF7njoPxKSu3lWroJaJJI2L3aBvelNyBjg1nkcfwWkpHldWeKyFnZtDDVfhO6Ut6+G2kNue3pOQyDnToAjxsfcqB4FtUg0fPImLLzdLcLbKxdO+j6Mu9zTu7KEUVnSMPpi63J311xV7ralpMMR4xpCOXeCm//FGG9rboeCWiRarIVP3oAFt8OWpdD7SHeHPfZSSE71urr4cGhfw9C4ug6/irqhcUluaFxeo86+7OMgyX8jjxXUItFmLWz4t2sS2VrkJoSa+iMYMxOSUryuzj+Cte4BpMadfTs/ABt02/sMajQCowAGjo6bUTgKapGuYq0bHbLgNje6oG++W4B31Axf3uVFlbVu7vDGzRfbVjYaGtenhaFx/bytOYp8E9Rz39nC4H7pjMnrQ2qypiERH7MWPn7ZBfb21ZA5NBTY0yGQ5HV13jiw2128ihvdLe8LTTeblAoDRoc6/BoNjYvxduVI8kVQV1XXMvrWVzhUE6R7ShITB2cyZWgWk4dkMTK3N0mBxDlhEkeshQ//5R6c2fEeZB/rAnvEV+M7sGsOukfyG3f4la1v2J59bNM75f4jE75N3xdBDVC+/xBLN+5i6cYyFm8o5eMdlQD0TEvmC4OzmDI0iylHZ3FsTk8CCm7xk2AQPvynC+ydH0C/YXDKzXD8V7rsgYqoCQZh14amExRtfw+C1W57Rv/QQyR1ozDGuyf+pAnfBHVzO/dWsXTjLpZsKGXxhjI2l+0HICs9lcKhoeAemk1+Vg9MAv0XSXwsGHSz9C28A0o/gpwRLrCHneufwK7c2XS88rYVUFXhtqWkhwK50dC4XrkJ1YRxuHwb1M0V797Pkg1lLNlQxuINZWzfUwXAwN5pTA41k0w5OpvcPnpSTGJcsBbef8YFdtk69+jyKbfAcWfHVqgdrAwNjWvU4VexxW0zSdB/eNNHrvsdF99NOlEUN0HdmLWWT0r3sXhDGUs2uvDetc89hXRUVg/Xvj00m8lDsujXUw8gSIwK1sJ789zUqrs2wsCxLrCPPaPrA7u2BkrWNh2vXLK20dC4oxoCOa/Adf5pvpOIicugbi4YtHy8cy+L17u77WUby9h7sAaAY/tnMGVoNpOHZlE4OIvePTSuVWJMbQ2sfgIW/RZ2b3KPOE/7CRx9WnQC21o3n3Lj8crbVkLNAbe9e9/PD41Lz458HVIvIYK6uZraIO9v28PiDa5j8p1Nu6iqDmIMjDyid+iOO4uJ+Zmkd9P4VokRtdWw6jF443/c03d5E11gD5nWucA+sLuh6aJ+aFxo2aikbu7BkfomjPEJNzQuFiRkUDd3qCbIyi3lofbtUt79tJxDtUGSA4YxR/apD+7xg/qSlqI2NvFYzSFYOQcW/Q72FMORhS6wB09tP0Crq2DHmqYdfrs2hDaahqFxdY9d54xI+KFxsUBB3YIDh2pZvnk3i0MjSlYXlxO0kJocoOCovvVt3KPzepOS5JPeeIk/NQfh3X/Aot/D3m1w1Ikw7RbIP9FtDwbd+OTG45W3r2k0NG5Ao1njCuCIsRoaF6M6FdTGmAeBc4Gd1tqR4fxCPwR1c3uqqnnnk12hppIy1n62B4D01CQmDc6sb+MePrCXxnBL16uughUPw5t/gMrtLrCTkmHru3AwNDQuNSM0a1yjduXeud7WLWHrbFBPBSqBR+I5qJvbte8QyzaW1bdxbyhx8w/07p5C4RAX3FOGZnF0TobGcEvXqT4ARQ/Bsr82LBNV99h19rEaGudjnW76MMbkA/MTKaib27Gnqr59+631ZWwtd73j2RndQg/euIdvjszsruAWkQ7rkqA2xswCZgEMGjRowubNmw+rWL/Ysmt/ffv24g1llOw9CEBun+71j7pPHpLNgN5a9UNE2qc76iiz1rKhZF/9o+5LNpZRvt915gzpl15/t104JIvMdPWui8jntRXUGkAcAcYYjs7J4OicDC6fnE8waFm7fU/9o+7PrNjKo0vdihTDBvSsb9+eNCSTXml6+EZE2qY76i5QXRvkva0V9W3cRZt2c7AmSMDAqLw+9W3cBUdl0j1VnUEiiaizoz4eA04BsoEdwC+stQ+09R4FdduqqmtZuaXcNZOEHr6pCVpSkgzjBvV1k0sNzWLsoD50S1ZwiyQCPfAS4/YdrKEo9PDNkg1lrNlaQdBCWkqAifmZTA61cY88ohfJevhGJC4pqH2mYn81yz5x7dtLN5bx4fa9APTslswXhmQyOdTGfVx/LaAgEi/UmegzvXukcPqIAZw+YgAApZUHQ6veuOlcX1vr1pnLTE9l8pCs+kUUhmSnawy3SBzSHbUPbSs/UD+iZMmGUrZVuAUU+vfqVv+o+5ShWeT11VzBIn6hpo84Zq1lc9n+RgsolFJa6RZQGJTZo35WwMlDs8jpqYdvRGKVgjqBWGtZt7OSxetL69u491S5BRSOycmonxWwcEgmfXro4RuRWKGgTmC1QcsH2/bUP+7+zqZd7D9UizEwfGCv+qcmJw7OJEMLKIh4RkEt9Q7VBFldXF7fxr38090cqgmSFDCMyetd/9Tk+KO0gIJIV1JQS6uqqmtZsXl3/XSuq4orqA1aUpMDTBjkFlAYldeb1OQAyYEASQFICgRIMoakgCE5yRAwhuSA+7nJlzEkJZn6fZMCbj+NTBH5PA3Pk1alpSQx5ehsphydDRxH5cGa0AIKrqnkD699TKSv5cbQEOymcbi7C0FyIECg7rup+9mFfN33li4Ure3T2gWkYZ9GF6D6CxEkJbkLUouf3exC1NI+AePqa62O5LoLXrN9AgZdzKQJBbU0kdEtmWnDcpg2LAeA8v2HWL+zkpqgpbbZV03QErQ2tC1IbZBm35vuEww2/d7S53xuH2uprQ19b7LNff7Bmlpq7ed/b22j9zat8/PHEIsaX1RauwjU7WMM9UFvjHEXG1P32gV/oP61+4xA6D31r+u2mdY/s/H2gCH0XtP0c+pfd+A9db8j9J6G16E6mtX2+c9pdnyN3xP6O2zrPc2PNRYpqKVNfXqkUpCf6XUZURUMNlwIaoMtBHqTi4W7INQEgwTrvltLTW3Dvi1dcOr3sS1c7Frcp4ULX93FKrSPqxuCoddBa6kNhn4O7Wst9XVV17qLWDD0nvrXoe0Nn0NoW8NnWttQe+PPtI0+J17UXSCbXiRo40LXcAHJSu/G3G9PjnhNCmpJeIGAIYBBfaed0zTwQxeDZuHfduA3voA0e0/9fi2/p+F30+ii1drn0OJntv45TS9+LV3c6i6YPaM0ckpBLSIRUXfBk8jTVGwiIjFOQS0iEuMU1CIiMU5BLSIS4xTUIiIxTkEtIhLjFNQiImNrbkoAAAObSURBVDFOQS0iEuOiMnueMaYE2HyYb88GSiNYjpfi5Vji5ThAxxKL4uU4oHPHcpS1tl9LG6IS1J1hjClqbao/v4mXY4mX4wAdSyyKl+OA6B2Lmj5ERGKcglpEJMbFYlDP9rqACIqXY4mX4wAdSyyKl+OAKB1LzLVRi4hIU7F4Ry0iIo0oqEVEYpwnQW2MOdMY85ExZr0x5uYWthtjzF2h7auNMeO9qDMcYRzLKcaYCmPMytDXz72osz3GmAeNMTuNMWta2e6nc9LesfjlnBxpjFlgjFlrjHnfGHNDC/v44ryEeSx+OS9pxpi3jTGrQsdyawv7RPa82NCSNl31BSQBG4AhQCqwChjebJ+zgRcBAxQCy7q6zggeyynAfK9rDeNYpgLjgTWtbPfFOQnzWPxyTgYC40OvewIf+/jfSjjH4pfzYoCM0OsUYBlQGM3z4sUd9SRgvbV2o7X2EPA48JVm+3wFeMQ6S4E+xpiBXV1oGMI5Fl+w1i4CdrWxi1/OSTjH4gvW2s+stStCr/cCa4HcZrv54ryEeSy+EPq7rgz9mBL6aj4qI6LnxYugzgW2NPq5mM+fsHD2iQXh1jk59N+kF40xI7qmtIjzyzkJl6/OiTEmHxiHu3trzHfnpY1jAZ+cF2NMkjFmJbATeNVaG9Xz4sXiti2tftn8ahTOPrEgnDpX4J7hrzTGnA08CxwT9coizy/nJBy+OifGmAzgKeBGa+2e5ptbeEvMnpd2jsU358VaWwuMNcb0AZ4xxoy01jbuE4noefHijroYOLLRz3nAtsPYJxa0W6e1dk/df5OstS8AKcaY7K4rMWL8ck7a5adzYoxJwQXbHGvt0y3s4pvz0t6x+Om81LHWlgMLgTObbYroefEiqN8BjjHGDDbGpAIzgeeb7fM8cEWo57QQqLDWftbVhYah3WMxxgwwxpjQ60m4v/OyLq+08/xyTtrll3MSqvEBYK219g+t7OaL8xLOsfjovPQL3UljjOkOnAZ82Gy3iJ6XLm/6sNbWGGOuB17GjZp40Fr7vjHm26HtfwVewPWargf2A1d1dZ3hCPNYpgPXGmNqgAPATBvqFo4lxpjHcL3u2caYYuAXuE4SX50TCOtYfHFOgBOAy4H3Qu2hAD8BBoHvzks4x+KX8zIQeNgYk4S7mMy11s6PZobpEXIRkRinJxNFRGKcglpEJMYpqEVEYpyCWkQkximoRURinIJaRCTGKahFRGLc/wGdHIUNqJyOXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 2)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t84.44444%\n",
      "\n",
      "\n",
      "\n",
      "The Classification Report is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       246\n",
      "           1       0.70      0.89      0.78       114\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.82      0.86      0.83       360\n",
      "weighted avg       0.86      0.84      0.85       360\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Accuracy:\\t{:0.5f}%'.format(accuracy_score(test_Y,Y_pred_labels)*100))\n",
    "print(\"\\n\\n\")\n",
    "print(\"The Classification Report is\")\n",
    "print(classification_report(test_Y, Y_pred_labels))\n",
    "print(\"\\n\\n\")\n",
    "# print(\"The Confusion Matrix is\")\n",
    "matrix = confusion_matrix(test_Y, Y_pred_labels)\n",
    "plot1 = sns.heatmap(matrix,annot=True,cbar=False,fmt='d')  \n",
    "plt.ylabel('True Value')  \n",
    "plt.xlabel('Predicted Value')  \n",
    "plt.title('Resnet Confusion Matrix')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
